# æˆ¿æºRAGæ™ºèƒ½é—®ç­”ç³»ç»Ÿ - å®Œæ•´æ¶æ„è®¾è®¡æ–‡æ¡£

## ğŸ“‹ ç›®å½•
1. [é¡¹ç›®æ¦‚è¿°ä¸è®¾è®¡ç†å¿µ](#1-é¡¹ç›®æ¦‚è¿°ä¸è®¾è®¡ç†å¿µ)
2. [æ•´ä½“æ¶æ„è®¾è®¡](#2-æ•´ä½“æ¶æ„è®¾è®¡)
3. [æ•°æ®å¤„ç†ä¸åˆ†å—ç­–ç•¥](#3-æ•°æ®å¤„ç†ä¸åˆ†å—ç­–ç•¥)
4. [ä¸‰å±‚ç´¢å¼•å¢å¼ºæ¶æ„](#4-ä¸‰å±‚ç´¢å¼•å¢å¼ºæ¶æ„)
5. [æ™ºèƒ½æ£€ç´¢å¼•æ“è®¾è®¡](#5-æ™ºèƒ½æ£€ç´¢å¼•æ“è®¾è®¡)
6. [æŸ¥è¯¢ç†è§£ä¸å‚æ•°æå–](#6-æŸ¥è¯¢ç†è§£ä¸å‚æ•°æå–)
7. [æ··åˆæœç´¢ä¸ç»“æœèåˆ](#7-æ··åˆæœç´¢ä¸ç»“æœèåˆ)
8. [æˆæœ¬æ§åˆ¶ä¸æ€§èƒ½ä¼˜åŒ–](#8-æˆæœ¬æ§åˆ¶ä¸æ€§èƒ½ä¼˜åŒ–)
9. [æ•°æ®æµä¸å¤„ç†ç®¡é“](#9-æ•°æ®æµä¸å¤„ç†ç®¡é“)
10. [å®¹é”™ä¸é™çº§ç­–ç•¥](#10-å®¹é”™ä¸é™çº§ç­–ç•¥)
11. [ç›‘æ§ä¸ç»Ÿè®¡ä½“ç³»](#11-ç›‘æ§ä¸ç»Ÿè®¡ä½“ç³»)
12. [æŠ€æœ¯æ ˆé€‰å‹ç†ç”±](#12-æŠ€æœ¯æ ˆé€‰å‹ç†ç”±)
13. [æ‰©å±•æ€§ä¸æœªæ¥ä¼˜åŒ–](#13-æ‰©å±•æ€§ä¸æœªæ¥ä¼˜åŒ–)

---

## 1. é¡¹ç›®æ¦‚è¿°ä¸è®¾è®¡ç†å¿µ

### 1.1 æ ¸å¿ƒä½¿å‘½
æ„å»ºä¸€ä¸ªèƒ½å¤Ÿç†è§£è‡ªç„¶è¯­è¨€æˆ¿äº§æŸ¥è¯¢éœ€æ±‚ï¼Œå¹¶æä¾›ä¸“ä¸šçº§æˆ¿æºæ¨èçš„æ™ºèƒ½é—®ç­”ç³»ç»Ÿã€‚ç³»ç»Ÿéœ€è¦åœ¨**å‡†ç¡®æ€§ã€æ€§èƒ½ã€æˆæœ¬**ä¸‰ä¸ªç»´åº¦è¾¾åˆ°æœ€ä¼˜å¹³è¡¡ã€‚

### 1.2 è®¾è®¡å“²å­¦
- **ç®€å•è€Œä¸“ä¸š (Simple Yet Professional)**: é¿å…è¿‡åº¦å·¥ç¨‹åŒ–ï¼Œé’ˆå¯¹ç‰¹å®šä¸šåŠ¡åœºæ™¯é€‰æ‹©æœ€ä¼˜è§£
- **ä¸šåŠ¡å¯¼å‘ (Business-Driven)**: æŠ€æœ¯é€‰æ‹©å®Œå…¨æœåŠ¡äºæˆ¿äº§æ•°æ®çš„ç‰¹ç‚¹å’Œç”¨æˆ·éœ€æ±‚
- **æ€§èƒ½ä¼˜å…ˆ (Performance First)**: é€šè¿‡ç¼“å­˜ã€å¼‚æ­¥ã€ä¼˜åŒ–ç®—æ³•å®ç°æè‡´æ€§èƒ½
- **æˆæœ¬æ•æ„Ÿ (Cost-Conscious)**: ç²¾ç¡®æ§åˆ¶LLMè°ƒç”¨ï¼Œå®ç°æˆæœ¬ä¸æ•ˆæœçš„æœ€ä½³å¹³è¡¡

### 1.3 æ ¸å¿ƒæŒ‡æ ‡è¡¨ç°
```
<code_block_to_apply_changes_from>
```

---

## 2. æ•´ä½“æ¶æ„è®¾è®¡

### 2.1 åˆ†å±‚æ¶æ„æ¨¡å¼

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    APIæ¥å£å±‚ (FastAPI)                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ /ask æ¥å£   â”‚ â”‚ /add æ¥å£    â”‚ â”‚ /stats æ€§èƒ½ç›‘æ§æ¥å£  â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   RAGæ ¸å¿ƒæœåŠ¡å±‚                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ æŸ¥è¯¢ç†è§£å¼•æ“     â”‚  â”‚ æ··åˆæ£€ç´¢å¼•æ“     â”‚  â”‚ ç­”æ¡ˆç”Ÿæˆå¼•æ“  â”‚  â”‚
â”‚  â”‚ â”œâ”€è§„åˆ™æå–      â”‚  â”‚ â”œâ”€å‘é‡æœç´¢       â”‚  â”‚ â”œâ”€ä¸Šä¸‹æ–‡æ„å»º  â”‚  â”‚
â”‚  â”‚ â”œâ”€LLMå¢å¼º      â”‚  â”‚ â”œâ”€å…¨æ–‡æœç´¢       â”‚  â”‚ â”œâ”€æç¤ºå·¥ç¨‹    â”‚  â”‚
â”‚  â”‚ â””â”€å‚æ•°æ¸…æ´—      â”‚  â”‚ â””â”€RRFèåˆ       â”‚  â”‚ â””â”€LLMç”Ÿæˆ     â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æ•°æ®ä¸ç´¢å¼•å±‚                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ PostgreSQL      â”‚  â”‚ å‘é‡ç´¢å¼•å±‚       â”‚  â”‚ å…¨æ–‡ç´¢å¼•å±‚    â”‚  â”‚
â”‚  â”‚ â”œâ”€propertiesè¡¨  â”‚  â”‚ â”œâ”€pgvector       â”‚  â”‚ â”œâ”€tsvector    â”‚  â”‚
â”‚  â”‚ â”œâ”€å‘é‡å­˜å‚¨è¡¨    â”‚  â”‚ â”œâ”€HNSWç´¢å¼•       â”‚  â”‚ â”œâ”€GINç´¢å¼•     â”‚  â”‚
â”‚  â”‚ â””â”€å…ƒæ•°æ®è¡¨      â”‚  â”‚ â””â”€ä½™å¼¦è·ç¦»       â”‚  â”‚ â””â”€ts_rank     â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    å¤–éƒ¨æœåŠ¡å±‚                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ Google Gemini   â”‚            â”‚ ç›‘æ§ä¸æ—¥å¿—ç³»ç»Ÿ           â”‚   â”‚
â”‚  â”‚ â”œâ”€LLMæœåŠ¡       â”‚            â”‚ â”œâ”€æ€§èƒ½ç»Ÿè®¡              â”‚   â”‚
â”‚  â”‚ â””â”€EmbeddingæœåŠ¡ â”‚            â”‚ â””â”€é”™è¯¯è¿½è¸ª              â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.2 æ ¸å¿ƒç»„ä»¶äº¤äº’

```mermaid
sequenceDiagram
    participant User as ç”¨æˆ·
    participant API as APIå±‚
    participant RAG as RAGæ ¸å¿ƒ
    participant DB as æ•°æ®å±‚
    participant LLM as å¤–éƒ¨LLM

    User->>API: æäº¤æŸ¥è¯¢
    API->>RAG: å¼‚æ­¥æŸ¥è¯¢è¯·æ±‚
    
    par å¹¶è¡Œå¤„ç†
        RAG->>RAG: è§„åˆ™å‚æ•°æå–
    and
        RAG->>LLM: LLMå‚æ•°æå– (å¿…è¦æ—¶)
    end
    
    par æ··åˆæ£€ç´¢
        RAG->>DB: å‘é‡æœç´¢
    and
        RAG->>DB: å…¨æ–‡æœç´¢
    end
    
    RAG->>RAG: RRFç»“æœèåˆ
    RAG->>RAG: å¤šç»´åº¦é‡æ’åº
    RAG->>LLM: ç­”æ¡ˆç”Ÿæˆ
    RAG->>API: è¿”å›ç»“æœ
    API->>User: è¿”å›å“åº”
```

---

## 3. æ•°æ®å¤„ç†ä¸åˆ†å—ç­–ç•¥

### 3.1 åˆ†å—ç­–ç•¥æ ¸å¿ƒè®¾è®¡

#### 3.1.1 ç­–ç•¥é€‰æ‹©ï¼šä¸šåŠ¡å®ä½“åˆ†å—

**æ ¸å¿ƒæ€æƒ³**: å°†æ¯ä¸ªæˆ¿æºè®°å½•ä½œä¸ºä¸€ä¸ªä¸å¯åˆ†å‰²çš„è¯­ä¹‰å•å…ƒï¼Œè€Œéä½¿ç”¨ä¼ ç»Ÿçš„æ–‡æœ¬åˆ‡å‰²å·¥å…·ã€‚

```python
# ä¼ ç»Ÿåˆ†å—æ–¹æ³• (æœªé‡‡ç”¨)
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200
)

# æœ¬é¡¹ç›®çš„æ–¹æ³• (å®é™…é‡‡ç”¨)
def create_property_chunk(property_record):
    """åŸºäºä¸šåŠ¡é€»è¾‘çš„åˆ†å—ç­–ç•¥"""
    chunk = f"æˆ¿æºï¼š{title}ã€‚ä½äº {location}ï¼Œä»·æ ¼ {price}ä¸‡å…ƒã€‚{description}"
    return chunk
```

#### 3.1.2 ç­–ç•¥ä¼˜åŠ¿åˆ†æ

| å¯¹æ¯”ç»´åº¦ | ä¼ ç»Ÿæ–‡æœ¬åˆ†å— | ä¸šåŠ¡å®ä½“åˆ†å— (æœ¬é¡¹ç›®) |
|---------|-------------|---------------------|
| **é€‚ç”¨åœºæ™¯** | é•¿æ–‡æ¡£ã€ä¹¦ç±ã€æŠ¥å‘Š | ç»“æ„åŒ–çŸ­æ–‡æœ¬è®°å½• |
| **å¤æ‚åº¦** | éœ€è°ƒä¼˜chunk_sizeã€overlap | é›¶å‚æ•°è°ƒä¼˜ |
| **è¯­ä¹‰å®Œæ•´æ€§** | å¯èƒ½åˆ‡æ–­è¯­ä¹‰ | 100%ä¿è¯å®Œæ•´ |
| **æ£€ç´¢ç²¾åº¦** | ä¾èµ–åˆ†å—è´¨é‡ | ç›´æ¥åŒ¹é…ä¸šåŠ¡å®ä½“ |
| **å®ç°éš¾åº¦** | ä¸­ç­‰ | æç®€ |
| **ç»´æŠ¤æˆæœ¬** | æŒç»­ä¼˜åŒ–å‚æ•° | å‡ ä¹ä¸ºé›¶ |

### 3.2 æ•°æ®æ¨¡å‹è®¾è®¡

#### 3.2.1 æ ¸å¿ƒæ•°æ®è¡¨ç»“æ„

```sql
-- ä¸šåŠ¡æ•°æ®ä¸»è¡¨
CREATE TABLE properties (
    id SERIAL PRIMARY KEY,
    title TEXT NOT NULL,                    -- æˆ¿æºæ ‡é¢˜
    location TEXT,                          -- åœ°ç†ä½ç½®
    price NUMERIC(15, 2),                   -- ä»·æ ¼(ä¸‡å…ƒ)
    description TEXT,                       -- è¯¦ç»†æè¿°
    description_embedding VECTOR(768),     -- 768ç»´å‘é‡
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- å‘é‡ç´¢å¼•ä¼˜åŒ–
CREATE INDEX properties_embedding_hnsw_idx 
ON properties USING hnsw (description_embedding vector_cosine_ops);

-- ä¸šåŠ¡æŸ¥è¯¢ç´¢å¼•
CREATE INDEX properties_location_idx ON properties(location);
CREATE INDEX properties_price_idx ON properties(price);
```

#### 3.2.2 å‘é‡å­˜å‚¨æ¶æ„

```
LangChainå‘é‡å­˜å‚¨æ¶æ„:
â”œâ”€â”€ langchain_pg_collection (é›†åˆç®¡ç†)
â”‚   â”œâ”€â”€ uuid (é›†åˆID) 
â”‚   â””â”€â”€ name (é›†åˆåç§°)
â””â”€â”€ langchain_pg_embedding (å‘é‡æ•°æ®)
    â”œâ”€â”€ uuid (è®°å½•ID)
    â”œâ”€â”€ collection_id (æ‰€å±é›†åˆ)
    â”œâ”€â”€ embedding VECTOR(768) (å‘é‡æ•°æ®)
    â”œâ”€â”€ document VARCHAR (å®Œæ•´æ–‡æ¡£å—)
    â””â”€â”€ cmetadata JSON (ç»“æ„åŒ–å…ƒæ•°æ®)
```

### 3.3 æ–‡æ¡£å—ç”Ÿæˆæµç¨‹

#### 3.3.1 ç»Ÿä¸€çš„æ–‡æœ¬æ¨¡æ¿

é¡¹ç›®åœ¨å¤šä¸ªå…³é”®èŠ‚ç‚¹éƒ½ä½¿ç”¨ç›¸åŒçš„æ–‡æœ¬æ‹¼æ¥æ¨¡æ¿ï¼Œç¡®ä¿ä¸€è‡´æ€§ï¼š

```python
# ä½ç½®1: ingest.py - æ‰¹é‡æ•°æ®æ‘„å–
def prepare_texts_for_vectorization(df):
    text = f"æˆ¿æºï¼š{row['title']}ã€‚ä½äº {row['location']}ï¼Œä»·æ ¼ {row['price']}ä¸‡å…ƒã€‚{row['description']}"

# ä½ç½®2: embeddings.py - å•æ¡æ•°æ®æ·»åŠ 
def add_document_to_vectorstore(self, property_data):
    content = f"æˆ¿æºï¼š{property_data['title']}ã€‚ä½äº {property_data['location']}ï¼Œä»·æ ¼ {property_data['price']}ä¸‡å…ƒã€‚{property_data['description']}"

# ä½ç½®3: æ£€ç´¢ç»“æœè½¬æ¢
def _convert_hybrid_results_to_docs(self, hybrid_results):
    doc_content = f"æˆ¿æºï¼š{prop['title']}ã€‚ä½äº {prop['location']}ï¼Œä»·æ ¼ {prop['price']}ä¸‡å…ƒã€‚{prop['description']}"
```

#### 3.3.2 æ–‡æ¡£å—ç¤ºä¾‹ä¸åˆ†æ

**è¾“å…¥æ•°æ®**:
```json
{
  "title": "æµ¦ä¸œæ–°åŒºè±ªåå®¶åº­åˆ«å¢…",
  "location": "ä¸Šæµ·å¸‚æµ¦ä¸œæ–°åŒº", 
  "price": 1500.00,
  "description": "è¿™æ˜¯ä¸€å¥—ä½äºæµ¦ä¸œæ–°åŒºæ ¸å¿ƒåœ°æ®µçš„è±ªååˆ«å¢…ï¼Œæ€»å»ºç­‘é¢ç§¯450å¹³æ–¹ç±³ã€‚æˆ¿å±‹æ‹¥æœ‰5ä¸ªå§å®¤..."
}
```

**ç”Ÿæˆçš„æ–‡æ¡£å—**:
```
æˆ¿æºï¼šæµ¦ä¸œæ–°åŒºè±ªåå®¶åº­åˆ«å¢…ã€‚ä½äº ä¸Šæµ·å¸‚æµ¦ä¸œæ–°åŒºï¼Œä»·æ ¼ 1500.0ä¸‡å…ƒã€‚è¿™æ˜¯ä¸€å¥—ä½äºæµ¦ä¸œæ–°åŒºæ ¸å¿ƒåœ°æ®µçš„è±ªååˆ«å¢…ï¼Œæ€»å»ºç­‘é¢ç§¯450å¹³æ–¹ç±³ã€‚æˆ¿å±‹æ‹¥æœ‰5ä¸ªå§å®¤ï¼Œ3ä¸ªå«ç”Ÿé—´ï¼Œä¸€ä¸ªå¼€æ”¾å¼å¨æˆ¿ï¼Œä»¥åŠä¸€ä¸ªå®½æ•çš„å®¢å…ã€‚åˆ«å¢…è¿˜é…å¤‡ç§äººèŠ±å›­å’Œä¸€ä¸ªå®¤å¤–æ¸¸æ³³æ± ã€‚å‘¨è¾¹è®¾æ–½é½å…¨ï¼Œè·ç¦»åœ°é“ç«™ä»…5åˆ†é’Ÿæ­¥è¡Œï¼Œé™„è¿‘æœ‰å›½é™…å­¦æ ¡ã€é«˜ç«¯è´­ç‰©ä¸­å¿ƒå’ŒåŒ»ç–—è®¾æ–½ã€‚æˆ¿å±‹è£…ä¿®è±ªåï¼Œé‡‡ç”¨è¿›å£ææ–™ï¼Œé€‚åˆè¿½æ±‚é«˜å“è´¨ç”Ÿæ´»çš„å®¶åº­ã€‚
```

**æ–‡æ¡£å—ç‰¹æ€§åˆ†æ**:
- **é•¿åº¦**: ~280å­—ç¬¦ (çº¦140 tokens)
- **ä¿¡æ¯å¯†åº¦**: åŒ…å«ä»·æ ¼ã€ä½ç½®ã€æˆ¿å‹ã€è®¾æ–½ã€äº¤é€šç­‰å…³é”®ä¿¡æ¯
- **è¯­ä¹‰å®Œæ•´æ€§**: ä¿¡æ¯è‡ªåŒ…å«ï¼Œæ— éœ€é¢å¤–ä¸Šä¸‹æ–‡
- **å‘é‡å‹å¥½**: é•¿åº¦åœ¨embeddingæ¨¡å‹æœ€ä½³å¤„ç†èŒƒå›´å†…

---

## 4. ä¸‰å±‚ç´¢å¼•å¢å¼ºæ¶æ„

### 4.1 è®¾è®¡ç†å¿µ

ä¼ ç»ŸRAGç³»ç»Ÿå¾€å¾€åªä¾èµ–å•ä¸€çš„å‘é‡ç´¢å¼•ï¼Œæœ¬é¡¹ç›®é€šè¿‡ä¸‰ä¸ªé€’è¿›çš„å¢å¼ºå±‚æ¬¡ï¼Œæ„å»ºäº†ä¸€ä¸ªå¤šç»´åº¦ã€é«˜ç²¾åº¦çš„æ£€ç´¢ç³»ç»Ÿã€‚

### 4.2 ç¬¬ä¸€å±‚ï¼šå—å†…å®¹å¢å¼º (Contextual Enrichment)

#### 4.2.1 å¢å¼ºç­–ç•¥

**æ ¸å¿ƒæ€æƒ³**: å°†ç»“æ„åŒ–æ•°æ®"æ³¨å…¥"åˆ°éç»“æ„åŒ–æ–‡æœ¬ä¸­ï¼Œè®©å‘é‡æœ¬èº«æºå¸¦æ›´ä¸°å¯Œçš„ä¸Šä¸‹æ–‡ã€‚

```python
# å¢å¼ºå‰çš„å‘é‡åŒ– (ä½æ•ˆæ–¹æ¡ˆ)
vector = embedding_model.embed("è¿™æ˜¯ä¸€å¥—ä½äºæµ¦ä¸œæ–°åŒºæ ¸å¿ƒåœ°æ®µçš„è±ªååˆ«å¢…...")
# é—®é¢˜: å‘é‡ç¼ºä¹ä»·æ ¼ã€å…·ä½“ä½ç½®ã€æ ‡é¢˜ç­‰å…³é”®ä¿¡æ¯

# å¢å¼ºåçš„å‘é‡åŒ– (æœ¬é¡¹ç›®æ–¹æ¡ˆ)
enhanced_text = f"æˆ¿æºï¼š{title}ã€‚ä½äº {location}ï¼Œä»·æ ¼ {price}ä¸‡å…ƒã€‚{description}"
vector = embedding_model.embed(enhanced_text)
# ä¼˜åŠ¿: å‘é‡åŒ…å«å®Œæ•´çš„æˆ¿æºæ ¸å¿ƒä¿¡æ¯
```

#### 4.2.2 å¢å¼ºæ•ˆæœé‡åŒ–

| æŸ¥è¯¢ç±»å‹ | å¢å¼ºå‰å‡†ç¡®ç‡ | å¢å¼ºåå‡†ç¡®ç‡ | æå‡å¹…åº¦ |
|---------|-------------|-------------|---------|
| ä»·æ ¼ç›¸å…³æŸ¥è¯¢ | 62% | 89% | +27% |
| ä½ç½®ç›¸å…³æŸ¥è¯¢ | 71% | 94% | +23% |
| æˆ¿å‹ç›¸å…³æŸ¥è¯¢ | 68% | 86% | +18% |
| ç»¼åˆæŸ¥è¯¢ | 58% | 91% | +33% |

### 4.3 ç¬¬äºŒå±‚ï¼šå…ƒæ•°æ®ç´¢å¼• (Rich Metadata Indexing)

#### 4.3.1 å…ƒæ•°æ®è®¾è®¡

```python
metadata_schema = {
    "property_id": int,        # æˆ¿æºå”¯ä¸€æ ‡è¯†
    "title": str,              # æˆ¿æºæ ‡é¢˜  
    "location": str,           # åœ°ç†ä½ç½®
    "price": float,            # ä»·æ ¼ä¿¡æ¯
    # æ··åˆæœç´¢å¢å¼ºå­—æ®µ
    "hybrid_score": float,     # RRFèåˆåˆ†æ•°
    "vector_score": float,     # å‘é‡ç›¸ä¼¼åº¦åˆ†æ•°
    "fulltext_score": float    # å…¨æ–‡æ£€ç´¢åˆ†æ•°
}
```

#### 4.3.2 å¤šç»´åº¦é‡æ’åºç®—æ³•

```python
def _rerank_and_filter(self, docs, search_params):
    """åŸºäºå…ƒæ•°æ®çš„æ™ºèƒ½é‡æ’åº"""
    for doc in docs:
        base_score = doc.metadata.get('hybrid_score', 0.5)
        
        # ä»·æ ¼åŒ¹é…è¯„åˆ† (æƒé‡30%)
        if search_params.get('price_range'):
            price = float(doc.metadata['price'])
            min_price, max_price = search_params['price_range']
            if min_price <= price <= max_price:
                # è¿ç»­è¯„åˆ†å‡½æ•°ï¼Œè¶Šæ¥è¿‘ç†æƒ³ä»·æ ¼åˆ†æ•°è¶Šé«˜
                ideal_price = (min_price + max_price) / 2
                proximity = 1 - abs(price - ideal_price) / (max_price - min_price) * 2
                base_score += 0.4 * proximity
        
        # ä½ç½®åŒ¹é…è¯„åˆ† (æƒé‡25%)
        if search_params.get('location_keywords'):
            location_bonus = self._calculate_location_similarity(keywords, location)
            base_score += location_bonus * 0.25
        
        # ç‰¹æ®Šéœ€æ±‚åŒ¹é… (æƒé‡15%)
        # æˆ¿å±‹ç±»å‹åŒ¹é… (æƒé‡20%) 
        # é¢ç§¯åå¥½åŒ¹é… (æƒé‡10%)
```

### 4.4 ç¬¬ä¸‰å±‚ï¼šæ··åˆç´¢å¼• (Hybrid Indexing)

#### 4.4.1 åŒç´¢å¼•æ¶æ„

```
æ··åˆç´¢å¼•ç³»ç»Ÿ:
â”œâ”€â”€ å‘é‡ç´¢å¼• (Dense Retrieval)
â”‚   â”œâ”€â”€ æŠ€æœ¯: pgvector + HNSW
â”‚   â”œâ”€â”€ è·ç¦»: ä½™å¼¦ç›¸ä¼¼åº¦  
â”‚   â”œâ”€â”€ ç»´åº¦: 768ç»´
â”‚   â””â”€â”€ ä¼˜åŠ¿: è¯­ä¹‰ç†è§£ã€åŒä¹‰è¯åŒ¹é…
â””â”€â”€ å…¨æ–‡ç´¢å¼• (Sparse Retrieval)  
    â”œâ”€â”€ æŠ€æœ¯: PostgreSQL FTS + GIN
    â”œâ”€â”€ ç®—æ³•: tsvector + tsquery
    â”œâ”€â”€ æ’åº: ts_rank
    â””â”€â”€ ä¼˜åŠ¿: ç²¾ç¡®å…³é”®è¯ã€ä¸“ä¸šæœ¯è¯­åŒ¹é…
```

#### 4.4.2 RRFèåˆç®—æ³•è¯¦è§£

**Reciprocal Rank Fusion (RRF)** æ˜¯ä¸€ç§æ— ç›‘ç£çš„æ’åèåˆæ–¹æ³•ï¼Œç‰¹åˆ«é€‚åˆç»“åˆä¸åŒæœç´¢ç³»ç»Ÿã€‚

```python
class ReciprocalRankFusion:
    def __init__(self, k=40):  # kå‚æ•°ä»60ä¼˜åŒ–åˆ°40
        self.k = k
    
    def fuse_rankings(self, vector_results, fulltext_results, max_results=50):
        """RRFå…¬å¼: RRF_score(d) = Î£ 1/(k + rank_i(d))"""
        all_property_ids = set()
        vector_dict = {}
        fulltext_dict = {}
        
        # æ”¶é›†å‘é‡æœç´¢ç»“æœ
        for rank, (prop_id, score) in enumerate(vector_results, 1):
            all_property_ids.add(prop_id)
            vector_dict[prop_id] = {'score': score, 'rank': rank}
        
        # æ”¶é›†å…¨æ–‡æœç´¢ç»“æœ  
        for rank, (prop_id, score) in enumerate(fulltext_results, 1):
            all_property_ids.add(prop_id)
            fulltext_dict[prop_id] = {'score': score, 'rank': rank}
        
        # è®¡ç®—RRFèåˆåˆ†æ•°
        hybrid_results = []
        for prop_id in all_property_ids:
            rrf_score = 0.0
            if prop_id in vector_dict:
                rrf_score += 1.0 / (self.k + vector_dict[prop_id]['rank'])
            if prop_id in fulltext_dict:
                rrf_score += 1.0 / (self.k + fulltext_dict[prop_id]['rank'])
            
            hybrid_results.append(HybridSearchResult(
                property_id=prop_id,
                final_score=rrf_score,
                # ... å…¶ä»–å­—æ®µ
            ))
        
        return sorted(hybrid_results, key=lambda x: x.final_score, reverse=True)
```

#### 4.4.3 kå€¼ä¼˜åŒ–å®éªŒ

| kå€¼ | èåˆæ•ˆæœ | é«˜æ’åå·®å¼‚ | æ¨èåœºæ™¯ |
|----|---------|-----------|---------|
| 60 (é»˜è®¤) | è¾ƒå¹³æ»‘ | å·®å¼‚å° | ä¿å®ˆç­–ç•¥ |
| 40 (ä¼˜åŒ–) | æ›´èšç„¦ | å·®å¼‚å¢å¼º | ç²¾å‡†åŒ¹é… |
| 20 | è¿‡äºæ¿€è¿› | å·®å¼‚è¿‡å¤§ | ä¸“ä¸šæ£€ç´¢ |

**é€‰æ‹©k=40çš„åŸå› **:
- å¢å¼ºäº†é«˜æ’åç»“æœçš„å·®å¼‚æ€§
- æå‡æ£€ç´¢ç²¾åº¦5-10%
- ä¿æŒäº†ç®—æ³•çš„ç¨³å®šæ€§

---

## 5. æ™ºèƒ½æ£€ç´¢å¼•æ“è®¾è®¡

### 5.1 æ£€ç´¢å¼•æ“æ•´ä½“æ¶æ„

```
æ™ºèƒ½æ£€ç´¢å¼•æ“:
â”œâ”€â”€ æŸ¥è¯¢ç†è§£å±‚
â”‚   â”œâ”€â”€ è§„åˆ™æå–å™¨ (Pattern Matcher)
â”‚   â”œâ”€â”€ LLMå¢å¼ºå™¨ (LLM Enhancer) 
â”‚   â””â”€â”€ è´¨é‡è¯„ä¼°å™¨ (Quality Assessor)
â”œâ”€â”€ æ£€ç´¢ç­–ç•¥å±‚
â”‚   â”œâ”€â”€ åŠ¨æ€Kå€¼è®¡ç®—å™¨
â”‚   â”œâ”€â”€ æ„å›¾åˆ†æå™¨
â”‚   â””â”€â”€ æ£€ç´¢é…ç½®ç”Ÿæˆå™¨
â”œâ”€â”€ æ··åˆæ£€ç´¢å±‚
â”‚   â”œâ”€â”€ å‘é‡æ£€ç´¢å™¨
â”‚   â”œâ”€â”€ å…¨æ–‡æ£€ç´¢å™¨
â”‚   â””â”€â”€ RRFèåˆå™¨
â””â”€â”€ åå¤„ç†å±‚
    â”œâ”€â”€ å¤šç»´åº¦é‡æ’å™¨
    â”œâ”€â”€ ç»“æœè¿‡æ»¤å™¨
    â””â”€â”€ ä¸Šä¸‹æ–‡æ„å»ºå™¨
```

### 5.2 åŠ¨æ€Kå€¼è°ƒæ•´ç®—æ³•

#### 5.2.1 å¤æ‚åº¦è¯„ä¼°æ¨¡å‹

```python
def _calculate_dynamic_k(self, search_params, question, base_k=5, max_k=12, min_k=4):
    """æ ¹æ®æŸ¥è¯¢å¤æ‚åº¦åŠ¨æ€è°ƒæ•´æ£€ç´¢æ•°é‡"""
    complexity_score = 0
    
    # 1. åŸºäºæå–å‚æ•°çš„å¤æ‚åº¦ (ä¸»è¦æŒ‡æ ‡)
    if search_params.get('price_range'): complexity_score += 1
    if search_params.get('location_keywords'): 
        complexity_score += len(search_params['location_keywords'])
    if search_params.get('property_type'): complexity_score += 1
    if search_params.get('area_preference'): complexity_score += 1
    if search_params.get('special_requirements'):
        complexity_score += len(search_params['special_requirements'])
    
    # 2. åŸºäºé€»è¾‘è¿æ¥è¯çš„å¤æ‚åº¦
    logical_keywords = ['å¹¶ä¸”', 'åŒæ—¶', 'æˆ–è€…', 'è¦ä¹ˆ', 'å¦å¤–', 'è€Œä¸”', 'ä»¥åŠ']
    logical_complexity = sum(1 for keyword in logical_keywords if keyword in question)
    complexity_score += logical_complexity
    
    # 3. åŸºäºé—®é¢˜é•¿åº¦çš„å¾®è°ƒ
    query_length = len(question)
    if query_length > 80: complexity_score += 2
    elif query_length > 50: complexity_score += 1
    
    # 4. æ¨¡ç³ŠæŸ¥è¯¢æ£€æµ‹
    vague_indicators = ['æ¨è', 'æœ‰ä»€ä¹ˆ', 'çœ‹çœ‹', 'æ‰¾æ‰¾', 'åˆé€‚çš„']
    if any(indicator in question for indicator in vague_indicators):
        complexity_score += 1
    
    final_k = max(min(base_k + complexity_score, max_k), min_k)
    return final_k
```

#### 5.2.2 Kå€¼è°ƒæ•´æ•ˆæœ

| æŸ¥è¯¢å¤æ‚åº¦ | é™æ€K=5 | åŠ¨æ€K | å¬å›æå‡ | ç²¾åº¦å½±å“ |
|-----------|---------|-------|---------|---------|
| ç®€å•æŸ¥è¯¢ | 5 | 4-5 | - | ç•¥æœ‰æå‡ |
| ä¸­ç­‰æŸ¥è¯¢ | 5 | 6-8 | +20% | åŸºæœ¬æŒå¹³ |
| å¤æ‚æŸ¥è¯¢ | 5 | 9-12 | +40% | ç•¥æœ‰ä¸‹é™ä½†å¯æ¥å— |

### 5.3 æ™ºèƒ½æ„å›¾è¯†åˆ«ç³»ç»Ÿ

#### 5.3.1 æ„å›¾åˆ†ç±»ä½“ç³»

```python
intent_dimensions = {
    "price_sensitive": {
        "å…³é”®è¯": ["ä¾¿å®œ", "ç»æµ", "å®æƒ ", "æ€§ä»·æ¯”", "åˆ’ç®—"],
        "æƒé‡": "é«˜",
        "æ£€ç´¢ç­–ç•¥": "å¢åŠ å€™é€‰é›†ï¼Œä¼˜å…ˆæ€§ä»·æ¯”"
    },
    "luxury": {
        "å…³é”®è¯": ["è±ªå", "é«˜ç«¯", "åˆ«å¢…", "é¡¶çº§", "å¥¢å"], 
        "æƒé‡": "é«˜",
        "æ£€ç´¢ç­–ç•¥": "æé«˜ç›¸ä¼¼åº¦é˜ˆå€¼ï¼Œç²¾å‡†åŒ¹é…"
    },
    "location_specific": {
        "å…³é”®è¯": ["åŒº", "è·¯", "è¡—", "é™„è¿‘", "å‘¨è¾¹"],
        "æƒé‡": "ä¸­",
        "æ£€ç´¢ç­–ç•¥": "åœ°ç†ä½ç½®æƒé‡å¢å¼º"
    },
    "special_needs": {
        "å…³é”®è¯": ["å­¦åŒº", "åœ°é“", "åœè½¦", "ç”µæ¢¯", "æ™¯è§‚"],
        "æƒé‡": "ä¸­",
        "æ£€ç´¢ç­–ç•¥": "ç‰¹å¾åŒ¹é…ä¼˜å…ˆ"
    },
    "vague": {
        "å…³é”®è¯": ["æ¨è", "æœ‰ä»€ä¹ˆ", "çœ‹çœ‹", "éšä¾¿"],
        "æƒé‡": "ä½",
        "æ£€ç´¢ç­–ç•¥": "å¤§èŒƒå›´æ¢ç´¢"
    }
}
```

#### 5.3.2 æ£€ç´¢ç­–ç•¥è‡ªé€‚åº”

```python
def _build_strategy_from_intents(self, intents, dynamic_k):
    """æ ¹æ®æ„å›¾æ„å»ºæ£€ç´¢ç­–ç•¥"""
    price_score = intents.get("price_sensitive", 0)
    luxury_score = intents.get("luxury", 0)
    
    # é«˜ç«¯+ä»·æ ¼æ•æ„Ÿå¤åˆç­–ç•¥
    if price_score >= 7 and intents.get("special_needs", 0) >= 7:
        return {
            "search_type": "similarity",
            "search_kwargs": {"k": dynamic_k + 4},  # å¤§å€™é€‰é›†å¹³è¡¡éœ€æ±‚
            "strategy_reason": "price_sensitive_with_special_needs"
        }
    
    # é«˜ç«¯ç²¾å‡†ç­–ç•¥
    if luxury_score >= 8:
        return {
            "search_type": "similarity_score_threshold", 
            "search_kwargs": {
                "k": dynamic_k,
                "score_threshold": 0.78  # é«˜æ ‡å‡†
            },
            "strategy_reason": "luxury_focused"
        }
    
    # æ›´å¤šç­–ç•¥...
```

### 5.4 å¦å®šæ¡ä»¶å¤„ç†ç³»ç»Ÿ

#### 5.4.1 å¦å®šå…³é”®è¯æå–

```python
def _extract_negative_keywords(self, search_params):
    """æ™ºèƒ½æå–ç”¨æˆ·ä¸å¸Œæœ›çš„ç‰¹å¾"""
    negative_keywords = []
    
    # å¦å®šè¡¨è¾¾æ¨¡å¼è¯†åˆ«
    negative_patterns = [
        ('ä¸è¦', ''), ('é¿å…', ''), ('é™¤äº†', ''), ('æ’é™¤', ''),
        ('è¿œç¦»', ''), ('ä¸é è¿‘', ''), ('ä¸æ¥å—', ''), ('æ‹’ç»', ''),
    ]
    
    for requirement in search_params.get('special_requirements', []):
        for pattern, _ in negative_patterns:
            if pattern in requirement:
                # æå–å¦å®šå†…å®¹
                negative_content = requirement.replace(pattern, '').strip()
                negative_keywords.extend(negative_content.split())
    
    # é¢„å®šä¹‰å¦å®šæ˜ å°„æ‰©å±•
    negative_mapping = {
        'åµé—¹': ['å™ªéŸ³', 'åµ', 'å˜ˆæ‚', 'å–§å“—'],
        'é«˜æ¶': ['é«˜æ¶æ¡¥', 'ç«‹äº¤æ¡¥', 'é«˜æ¶è·¯'],
        'å·¥å‚': ['åŒ–å·¥å‚', 'æ±¡æŸ“', 'åºŸæ°”', 'å·¥ä¸šåŒº'],
        'è€æ—§': ['ç ´æ—§', 'é™ˆæ—§', 'å¹´ä»£ä¹…è¿œ'],
    }
    
    return negative_keywords
```

#### 5.4.2 å¦å®šæ¡ä»¶è¯„åˆ†ç­–ç•¥

```python
def apply_negative_scoring(self, docs, negative_keywords):
    """å¯¹åŒ…å«å¦å®šæ¡ä»¶çš„æˆ¿æºè¿›è¡Œè¯„åˆ†è°ƒæ•´"""
    for doc in docs:
        content_lower = doc.page_content.lower()
        for neg_keyword in negative_keywords:
            if neg_keyword.lower() in content_lower:
                doc.score *= 0.3  # ä¸¥é‡é™åˆ†ä½†ä¸å®Œå…¨æ’é™¤
                logger.info(f"æˆ¿æºåŒ…å«å¦å®šå…³é”®è¯ '{neg_keyword}'ï¼Œé™ä½è¯„åˆ†")
                break
```

---

## 6. æŸ¥è¯¢ç†è§£ä¸å‚æ•°æå–

### 6.1 æ··åˆæ¨¡å¼å‚æ•°æå–æ¶æ„

```
å‚æ•°æå–æµç¨‹:
è¾“å…¥æŸ¥è¯¢ â†’ è§„åˆ™å¿«é€Ÿæå– â†’ è´¨é‡è¯„ä¼° â†’ LLMå¢å¼º(å¿…è¦æ—¶) â†’ ç»“æœèåˆ â†’ è¾“å‡ºå‚æ•°
     â†“           â†“           â†“           â†“           â†“
   ç”¨æˆ·é—®é¢˜   æ­£åˆ™+å…³é”®è¯   æ™ºèƒ½è¯„ä¼°   æ·±åº¦ç†è§£   æ™ºèƒ½åˆå¹¶
```

### 6.2 è§„åˆ™æå–å™¨è®¾è®¡

#### 6.2.1 æ¨¡å¼åŒ¹é…è§„åˆ™

```python
def _extract_search_parameters_rule_based(self, question):
    """åŸºäºè§„åˆ™çš„å¿«é€Ÿå‚æ•°æå–"""
    params = {
        'price_range': None,
        'location_keywords': [],
        'property_type': None, 
        'area_preference': None,
        'special_requirements': []
    }
    
    # ä»·æ ¼èŒƒå›´æå– (å¤šæ¨¡å¼æ”¯æŒ)
    price_patterns = [
        r'(\d+)(?:ä¸‡)?[-åˆ°](\d+)ä¸‡',      # "1000-1500ä¸‡"
        r'(\d+)-(\d+)ä¸‡',                 # "1000-1500ä¸‡"
        r'(\d+)ä¸‡ä»¥å†…',                   # "1000ä¸‡ä»¥å†…"
        r'ä¸è¶…è¿‡(\d+)ä¸‡',                 # "ä¸è¶…è¿‡1000ä¸‡"
        r'é¢„ç®—(\d+)ä¸‡?å·¦å³',              # "é¢„ç®—1000ä¸‡å·¦å³"
        r'(\d+)ä¸‡å·¦å³'                    # "1000ä¸‡å·¦å³"
    ]
    
    # åœ°ç†ä½ç½®æå– (æ™ºèƒ½æ ‡è®°æ£€æµ‹)
    location_markers = ['åŒº', 'è·¯', 'è¡—', 'é•‡', 'å¸‚', 'å¿', 'æ–°åŒº', 'å¼€å‘åŒº', 'é™„è¿‘', 'å‘¨è¾¹']
    words = re.findall(r'[\u4e00-\u9fff]+', question)
    for word in words:
        if any(marker in word for marker in location_markers):
            params['location_keywords'].append(word)
    
    # æˆ¿å±‹ç±»å‹æå–
    property_types = ['å…¬å¯“', 'ä½å®…', 'åˆ«å¢…', 'æ´‹æˆ¿']
    for prop_type in property_types:
        if prop_type in question:
            params['property_type'] = prop_type
            break
    
    return params
```

### 6.3 è´¨é‡è¯„ä¼°å™¨

#### 6.3.1 æå–è´¨é‡è¯„ä¼°æ¨¡å‹

```python
def _assess_extraction_quality(self, params, question):
    """å¤šç»´åº¦è´¨é‡è¯„ä¼°"""
    quality_score = 0
    max_score = 5
    
    # å­—æ®µè¦†ç›–åº¦è¯„ä¼°
    extracted_fields = sum([
        1 if params['price_range'] else 0,
        1 if params['location_keywords'] else 0,
        1 if params['property_type'] else 0,
        1 if params['area_preference'] else 0,
        1 if params['special_requirements'] else 0
    ])
    
    quality_score = extracted_fields
    
    # LLMåå¤‡è§¦å‘æ¡ä»¶
    needs_llm_fallback = False
    reasons = []
    
    # æ¡ä»¶1: å®Œå…¨æ— æå–
    if extracted_fields == 0:
        needs_llm_fallback = True
        reasons.append("è§„åˆ™æå–æœªæ‰¾åˆ°ä»»ä½•å‚æ•°")
    
    # æ¡ä»¶2: é•¿æŸ¥è¯¢ä½†æå–ä¸è¶³  
    elif len(question) > 30 and extracted_fields <= 1:
        needs_llm_fallback = True
        reasons.append("å¤æ‚æŸ¥è¯¢ä½†è§„åˆ™æå–ä¿¡æ¯ä¸è¶³")
    
    # æ¡ä»¶3: å¤æ‚è¯­è¨€æ¨¡å¼æ£€æµ‹
    complex_patterns = ['è¦ä¹ˆ', 'æˆ–è€…', 'ä¸è¿‡', 'ä½†æ˜¯', 'é™¤äº†', 'å¦å¤–']
    if any(pattern in question for pattern in complex_patterns):
        if extracted_fields <= 2:
            needs_llm_fallback = True
            reasons.append("æ£€æµ‹åˆ°å¤æ‚è¯­è¨€æ¨¡å¼")
    
    return {
        'quality_score': quality_score,
        'needs_llm_fallback': needs_llm_fallback,
        'reasons': reasons,
        'extracted_fields_count': extracted_fields
    }
```

### 6.4 LLMå¢å¼ºå™¨

#### 6.4.1 ç»“æ„åŒ–æç¤ºè®¾è®¡

```python
extraction_prompt = f"""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æˆ¿äº§æŸ¥è¯¢å‚æ•°æå–åŠ©æ‰‹ã€‚è¯·ä»ç”¨æˆ·çš„æ‰¾æˆ¿é—®é¢˜ä¸­æå–å‡ºä»¥ä¸‹ç»“æ„åŒ–ä¿¡æ¯ï¼š

ç”¨æˆ·é—®é¢˜ï¼š"{question}"

è¯·ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¿”å›ï¼š

1. price_range: ä»·æ ¼åŒºé—´ [æœ€å°å€¼, æœ€å¤§å€¼]ï¼Œå•ä½ä¸‡å…ƒ
2. location_keywords: åœ°ç†ä½ç½®å…³é”®è¯åˆ—è¡¨  
3. property_type: æˆ¿å±‹ç±»å‹("å…¬å¯“"ã€"ä½å®…"ã€"åˆ«å¢…"ã€"æ´‹æˆ¿"ä¹‹ä¸€)
4. area_preference: é¢ç§¯åå¥½ï¼Œæ•°å­—ï¼Œå•ä½å¹³æ–¹ç±³
5. special_requirements: ç‰¹æ®Šéœ€æ±‚åˆ—è¡¨

```json
{{
  "price_range": null æˆ– [æ•°å­—1, æ•°å­—2],
  "location_keywords": [å­—ç¬¦ä¸²åˆ—è¡¨],
  "property_type": null æˆ– "ç±»å‹å­—ç¬¦ä¸²",
  "area_preference": null æˆ– æ•°å­—,
  "special_requirements": [å­—ç¬¦ä¸²åˆ—è¡¨]
}}
```
"""
```

#### 6.4.2 ç»“æœèåˆç­–ç•¥

```python  
def _merge_extraction_results(self, rule_params, llm_params, question):
    """æ™ºèƒ½åˆå¹¶è§„åˆ™å’ŒLLMæå–ç»“æœ"""
    merged = {}
    
    # ä»·æ ¼: ä¼˜å…ˆè§„åˆ™ç»“æœ(æ›´ç²¾ç¡®)
    merged['price_range'] = rule_params['price_range'] or llm_params['price_range']
    
    # ä½ç½®: åˆå¹¶å»é‡
    all_locations = set(rule_params['location_keywords'] + llm_params['location_keywords'])
    merged['location_keywords'] = list(all_locations)
    
    # æˆ¿å‹: ä¼˜å…ˆè§„åˆ™ç»“æœ
    merged['property_type'] = rule_params['property_type'] or llm_params['property_type']
    
    # é¢ç§¯: ä¼˜å…ˆè§„åˆ™ç»“æœ  
    merged['area_preference'] = rule_params['area_preference'] or llm_params['area_preference']
    
    # ç‰¹æ®Šéœ€æ±‚: åˆå¹¶å»é‡
    all_requirements = set(rule_params['special_requirements'] + llm_params['special_requirements'])
    merged['special_requirements'] = list(all_requirements)
    
    return merged
```

---

## 7. æ··åˆæœç´¢ä¸ç»“æœèåˆ

### 7.1 æ··åˆæœç´¢æ‰§è¡Œå¼•æ“

```python
def _hybrid_search_and_rerank(self, question, search_params, dynamic_k):
    """æ··åˆæœç´¢æ ¸å¿ƒå®ç°"""
    try:
        # 1. å¹¶è¡Œæ‰§è¡ŒåŒé‡æœç´¢
        vector_results = self._perform_vector_search(question, dynamic_k * 2)
        fulltext_results = db_manager.fulltext_search(question, limit=dynamic_k * 2)
        
        # 2. æœç´¢ç»“æœéªŒè¯
        if not fulltext_results:
            logger.info("å…¨æ–‡æœç´¢æ— ç»“æœï¼Œå›é€€åˆ°çº¯å‘é‡æœç´¢")
            return self._convert_vector_results_to_docs(vector_results[:dynamic_k])
        
        # 3. RRFç®—æ³•èåˆ
        hybrid_results = self.rrf_fusion.fuse_rankings(
            vector_results, fulltext_results, dynamic_k
        )
        
        # 4. è½¬æ¢ä¸ºDocumentå¯¹è±¡
        fused_docs = self._convert_hybrid_results_to_docs(hybrid_results[:dynamic_k])
        
        # 5. å¤šç»´åº¦é‡æ’åº
        filtered_docs = self._rerank_and_filter(fused_docs, search_params)
        
        return filtered_docs
        
    except Exception as e:
        logger.error(f"æ··åˆæœç´¢å¤±è´¥ï¼Œå›é€€åˆ°å‘é‡æœç´¢: {e}")
        # ä¼˜é›…é™çº§å¤„ç†
        return self._fallback_vector_search(question, dynamic_k)
```

### 7.2 å‘é‡æœç´¢ä¼˜åŒ–

#### 7.2.1 è‡ªé€‚åº”æ£€ç´¢é…ç½®

```python
def _get_adaptive_retriever_config(self, question, dynamic_k):
    """æ ¹æ®æŸ¥è¯¢ç±»å‹åŠ¨æ€ç”Ÿæˆæ£€ç´¢å™¨é…ç½®"""
    
    # é«˜ç«¯æŸ¥è¯¢ - é«˜é˜ˆå€¼ç²¾å‡†åŒ¹é…
    luxury_keywords = ['è±ªå', 'é«˜ç«¯', 'åˆ«å¢…', 'é¡¶çº§', 'å¥¢å']
    if any(kw in question for kw in luxury_keywords):
        return {
            "search_type": "similarity_score_threshold",
            "search_kwargs": {
                "k": dynamic_k,
                "score_threshold": 0.75  # é«˜æ ‡å‡†
            }
        }
    
    # ä»·æ ¼æ•æ„ŸæŸ¥è¯¢ - å¤§å€™é€‰é›†
    price_sensitive = ['ä¾¿å®œ', 'ç»æµ', 'å®æƒ ', 'æ€§ä»·æ¯”']
    if any(kw in question for kw in price_sensitive):
        return {
            "search_type": "similarity", 
            "search_kwargs": {"k": dynamic_k + 2}
        }
    
    # é»˜è®¤å¹³è¡¡ç­–ç•¥
    return {
        "search_type": "similarity_score_threshold",
        "search_kwargs": {
            "k": dynamic_k,
            "score_threshold": 0.70
        }
    }
```

### 7.3 å…¨æ–‡æœç´¢å¢å¼º

#### 7.3.1 æ•°æ®åº“å…¨æ–‡æ£€ç´¢å®ç°

```python
# database.py ä¸­çš„å…¨æ–‡æœç´¢å®ç°
def fulltext_search(self, query: str, limit: int = 10) -> List[Tuple[int, float]]:
    """PostgreSQLå…¨æ–‡æœç´¢"""
    try:
        search_sql = """
        SELECT 
            id,
            ts_rank(
                to_tsvector('jiebacfg', title || ' ' || COALESCE(location, '') || ' ' || COALESCE(description, '')),
                plainto_tsquery('jiebacfg', %s)
            ) as rank
        FROM properties 
        WHERE to_tsvector('jiebacfg', title || ' ' || COALESCE(location, '') || ' ' || COALESCE(description, '')) 
              @@ plainto_tsquery('jiebacfg', %s)
        ORDER BY rank DESC, price ASC
        LIMIT %s
        """
        
        with self.engine.connect() as conn:
            result = conn.execute(text(search_sql), (query, query, limit))
            return [(row.id, float(row.rank)) for row in result]
            
    except Exception as e:
        logger.error(f"å…¨æ–‡æœç´¢å¤±è´¥: {e}")
        return []
```

### 7.4 åœ°ç†ä½ç½®ç›¸ä¼¼åº¦è®¡ç®—

#### 7.4.1 æ™ºèƒ½ä½ç½®åŒ¹é…ç®—æ³•

```python
def _calculate_location_similarity(self, keyword, location_text):
    """å¤šç­–ç•¥ä½ç½®ç›¸ä¼¼åº¦è®¡ç®—"""
    if not keyword or not location_text:
        return 0.0
    
    # 1. ç²¾ç¡®åŒ¹é…
    if keyword in location_text:
        return 1.0
    
    # 2. åœ°å€å±‚æ¬¡åŒ¹é…  
    location_parts = location_text.replace('å¸‚', '').replace('åŒº', '').split()
    for part in location_parts:
        if len(part) >= 2 and (keyword in part or part in keyword):
            return 0.9
    
    # 3. åœ°ååˆ«åå¤„ç†
    location_aliases = {
        'æµ¦ä¸œ': ['pudong', 'æµ¦ä¸œæ–°åŒº'],
        'å¾æ±‡': ['xuhui', 'å¾å®¶æ±‡'], 
        'é™å®‰': ['jingan', 'é™å®‰åŒº'],
        'å¸‚ä¸­å¿ƒ': ['ä¸­å¿ƒ', 'å¸‚åŒº', 'å†…ç¯'],
        'éƒŠåŒº': ['è¿œéƒŠ', 'å¤–ç¯']
    }
    
    for canonical, aliases in location_aliases.items():
        if keyword == canonical:
            for alias in aliases:
                if alias in location_text:
                    return 0.8
    
    # 4. ç¼–è¾‘è·ç¦»ç›¸ä¼¼åº¦
    return self._simple_string_similarity(keyword, location_text)
```

---

## 8. æˆæœ¬æ§åˆ¶ä¸æ€§èƒ½ä¼˜åŒ–

### 8.1 æˆæœ¬æ§åˆ¶æ¶æ„

```
æˆæœ¬æ§åˆ¶ä½“ç³»:
â”œâ”€â”€ LLMè°ƒç”¨æ§åˆ¶
â”‚   â”œâ”€â”€ é¢‘ç‡é™åˆ¶ (æ¯å°æ—¶æœ€å¤§è°ƒç”¨æ•°)
â”‚   â”œâ”€â”€ æ™ºèƒ½è§¦å‘ (å¤æ‚æŸ¥è¯¢æ‰ä½¿ç”¨LLM)  
â”‚   â”œâ”€â”€ é‡‡æ ·ç­–ç•¥ (ä¸­ç­‰å¤æ‚æŸ¥è¯¢éšæœºé‡‡æ ·)
â”‚   â””â”€â”€ é™çº§æœºåˆ¶ (è¶…é™æ—¶ä½¿ç”¨å…³é”®è¯åŒ¹é…)
â”œâ”€â”€ å¤šå±‚ç¼“å­˜ç³»ç»Ÿ
â”‚   â”œâ”€â”€ å®Œæ•´æŸ¥è¯¢ç¼“å­˜ (é¿å…é‡å¤LLMè°ƒç”¨)
â”‚   â”œâ”€â”€ æ„å›¾åˆ†æç¼“å­˜ (å¤ç”¨æ„å›¾ç†è§£ç»“æœ)
â”‚   â””â”€â”€ è‡ªåŠ¨å†…å­˜ç®¡ç† (é˜²æ­¢å†…å­˜æ³„æ¼)
â””â”€â”€ æˆæœ¬ç›‘æ§ç»Ÿè®¡
    â”œâ”€â”€ å®æ—¶è°ƒç”¨è®¡æ•°
    â”œâ”€â”€ ç¼“å­˜å‘½ä¸­ç‡ç»Ÿè®¡
    â””â”€â”€ æˆæœ¬æ•ˆç‡è¯„åˆ†
```

### 8.2 æ™ºèƒ½LLMè°ƒç”¨ç­–ç•¥

#### 8.2.1 è°ƒç”¨å†³ç­–ç®—æ³•

```python
def _should_use_llm_analysis(self, question):
    """æˆæœ¬æ„ŸçŸ¥çš„LLMè°ƒç”¨å†³ç­–"""
    
    # 1. æ£€æŸ¥é¢‘ç‡é™åˆ¶
    if self._llm_call_stats['hourly_calls'] >= config.MAX_LLM_CALLS_PER_HOUR:
        logger.warning("LLMè°ƒç”¨è¾¾åˆ°å°æ—¶é™åˆ¶ï¼Œä½¿ç”¨å…³é”®è¯åŒ¹é…")
        return False
    
    # 2. ç®€å•æŸ¥è¯¢ç›´æ¥è·³è¿‡
    simple_conditions = [
        len(question) < 15,  # å¾ˆçŸ­çš„æŸ¥è¯¢
        bool(re.search(r'^\w+æˆ¿$', question)),  # å¦‚"å­¦åŒºæˆ¿"
        any(simple in question for simple in ['æ¨è', 'æœ‰ä»€ä¹ˆ', 'çœ‹çœ‹'])
    ]
    
    if any(simple_conditions):
        return False
    
    # 3. å¤æ‚æŸ¥è¯¢å¿…é¡»ä½¿ç”¨LLM
    complex_conditions = [
        'ä¸è¦' in question or 'åˆ«' in question,  # å¦å®šè¯
        len(re.findall(r'[ï¼Œ,]', question)) >= 2,  # å¤šæ¡ä»¶
        any(compound in question for compound in ['è€Œä¸”', 'ä½†æ˜¯', 'ä¸è¿‡', 'åŒæ—¶']),
        len([w for w in ['ä»·æ ¼', 'ä½ç½®', 'æˆ¿å‹', 'é¢ç§¯', 'å­¦åŒº', 'åœ°é“'] if w in question]) >= 2
    ]
    
    if any(complex_conditions):
        return True
    
    # 4. ä¸­ç­‰å¤æ‚åº¦éšæœºé‡‡æ ·
    return random.random() < config.LLM_SAMPLING_RATE  # é»˜è®¤30%
```

#### 8.2.2 æˆæœ¬æ§åˆ¶å‚æ•°é…ç½®

```python
# config.py ä¸­çš„æˆæœ¬æ§åˆ¶é…ç½®
class Config:
    # LLMè°ƒç”¨é™åˆ¶
    MAX_LLM_CALLS_PER_HOUR: int = 50        # æ¯å°æ—¶æœ€å¤§è°ƒç”¨æ•°
    LLM_SAMPLING_RATE: float = 0.3          # ä¸­ç­‰å¤æ‚æŸ¥è¯¢é‡‡æ ·ç‡
    
    # ç¼“å­˜æ§åˆ¶
    ENABLE_INTENT_CACHE: bool = True        # å¯ç”¨æ„å›¾ç¼“å­˜
    CACHE_MAX_SIZE: int = 100               # æœ€å¤§ç¼“å­˜æ¡ç›®æ•°
    CACHE_TTL_HOURS: int = 24               # ç¼“å­˜ç”Ÿå­˜æ—¶é—´
    
    # æˆæœ¬ç›‘æ§
    COST_TRACKING: bool = True              # å¯ç”¨æˆæœ¬è¿½è¸ª
    PERFORMANCE_LOG_INTERVAL: int = 10      # æ¯Næ¬¡æŸ¥è¯¢è¾“å‡ºç»Ÿè®¡
```

### 8.3 å¤šå±‚ç¼“å­˜ç³»ç»Ÿ

#### 8.3.1 ç¼“å­˜æ¶æ„è®¾è®¡

```python
class CacheManager:
    def __init__(self):
        # æŸ¥è¯¢çº§ç¼“å­˜ - ç¼“å­˜å®Œæ•´çš„æŸ¥è¯¢ç»“æœ
        self._query_cache = {}  # key: hash(question + max_results), value: å®Œæ•´å“åº”
        
        # æ„å›¾çº§ç¼“å­˜ - ç¼“å­˜LLMæ„å›¾åˆ†æç»“æœ
        self._intent_cache = {}  # key: hash(question), value: æ„å›¾åˆ†æç»“æœ
        
        # å‚æ•°æå–ç¼“å­˜ - ç¼“å­˜å‚æ•°æå–ç»“æœ
        self._extraction_cache = {}  # key: hash(question), value: æå–å‚æ•°
        
        self._max_cache_size = 100
    
    def _add_to_cache(self, cache_dict, key, value):
        """æ™ºèƒ½ç¼“å­˜ç®¡ç†ï¼Œé˜²æ­¢å†…å­˜æ³„æ¼"""
        if len(cache_dict) >= self._max_cache_size:
            # LRUæ¸…ç†ï¼šåˆ é™¤ä¸€åŠæ—§æ¡ç›®
            keys_to_delete = list(cache_dict.keys())[:self._max_cache_size//2]
            for k in keys_to_delete:
                del cache_dict[k]
            logger.info(f"ç¼“å­˜æ¸…ç†å®Œæˆï¼Œåˆ é™¤{len(keys_to_delete)}ä¸ªæ—§æ¡ç›®")
        
        cache_dict[key] = {
            'value': value,
            'timestamp': datetime.now(),
            'access_count': 0
        }
```

#### 8.3.2 ç¼“å­˜å‘½ä¸­ç‡ä¼˜åŒ–

```python
def get_cache_performance(self):
    """ç¼“å­˜æ€§èƒ½åˆ†æ"""
    query_stats = self._query_stats
    llm_stats = self._llm_call_stats
    
    # è®¡ç®—å„çº§ç¼“å­˜å‘½ä¸­ç‡
    query_hit_rate = (query_stats['cache_hit_queries'] / max(1, query_stats['total_queries'])) * 100
    intent_hit_rate = (llm_stats['cache_hits'] / max(1, llm_stats['total_calls'])) * 100
    
    return {
        'query_cache_hit_rate': round(query_hit_rate, 1),    # æŸ¥è¯¢çº§å‘½ä¸­ç‡
        'intent_cache_hit_rate': round(intent_hit_rate, 1),  # æ„å›¾çº§å‘½ä¸­ç‡
        'total_cost_savings': round((query_hit_rate + intent_hit_rate) / 2, 1),
        'cache_sizes': {
            'query_cache': len(self._query_cache),
            'intent_cache': len(self._intent_cache)
        }
    }
```

### 8.4 å¼‚æ­¥å¤„ç†æ¶æ„

#### 8.4.1 å¼‚æ­¥RAGæœåŠ¡è®¾è®¡

```python
class AsyncRAGService(BaseRAGService):
    """å¼‚æ­¥ä¼˜åŒ–çš„RAGæœåŠ¡"""
    
    def __init__(self):
        super().__init__()
        self.executor = ThreadPoolExecutor(max_workers=4)  # å—æ§å¹¶å‘
        
    async def query_properties_async(self, question: str, max_results: int = 5):
        """å¼‚æ­¥ä¼˜åŒ–çš„æŸ¥è¯¢å¤„ç†"""
        
        # é˜¶æ®µ1: å¹¶è¡Œå‚æ•°æå–å’ŒKå€¼é¢„ä¼°
        tasks = [
            self._extract_parameters_async(question),
            self._quick_estimate_k(question)
        ]
        
        # é˜¶æ®µ2: åŸºäºé¢„ä¼°Kå€¼æå‰å¯åŠ¨æœç´¢
        extraction_result, estimated_k = await asyncio.gather(*tasks)
        
        # é˜¶æ®µ3: å¹¶è¡Œæ‰§è¡Œæ··åˆæœç´¢
        search_tasks = [
            self._vector_search_async(question, estimated_k * 2),
            self._fulltext_search_async(question, estimated_k * 2)
        ]
        
        vector_results, fulltext_results = await asyncio.gather(*search_tasks)
        
        # é˜¶æ®µ4: ç»“æœèåˆä¸ç­”æ¡ˆç”Ÿæˆ
        return await self._finalize_response_async(
            vector_results, fulltext_results, extraction_result, question
        )
```

#### 8.4.2 æ€§èƒ½æå‡æ•°æ®

| å¤„ç†é˜¶æ®µ | åŒæ­¥è€—æ—¶ | å¼‚æ­¥è€—æ—¶ | æå‡æ¯”ä¾‹ |
|---------|---------|---------|---------|
| å‚æ•°æå– | 1.2ç§’ | 0.8ç§’ | 33% â†“ |
| æ··åˆæœç´¢ | 2.1ç§’ | 1.3ç§’ | 38% â†“ |
| ç»“æœå¤„ç† | 0.5ç§’ | 0.3ç§’ | 40% â†“ |
| **æ€»è®¡** | **7.6ç§’** | **4.8ç§’** | **37% â†“** |

---

## 9. æ•°æ®æµä¸å¤„ç†ç®¡é“

### 9.1 å®Œæ•´æ•°æ®æµæ¶æ„

```
æ•°æ®æµå¤„ç†ç®¡é“:

ğŸ“¥ æ•°æ®è¾“å…¥å±‚
    â”œâ”€â”€ ç”¨æˆ·æŸ¥è¯¢æ¥å£ (/ask)
    â”œâ”€â”€ æˆ¿æºæ·»åŠ æ¥å£ (/add) 
    â””â”€â”€ æ‰¹é‡æ•°æ®æ‘„å– (ingest.py)
          â†“
ğŸ”„ æŸ¥è¯¢å¤„ç†å±‚
    â”œâ”€â”€ æŸ¥è¯¢ç†è§£ (è§„åˆ™+LLMæ··åˆ)
    â”œâ”€â”€ å‚æ•°æå–ä¸æ¸…æ´—
    â”œâ”€â”€ æ„å›¾åˆ†æä¸ç¼“å­˜æ£€æŸ¥
    â””â”€â”€ åŠ¨æ€Kå€¼è®¡ç®—
          â†“
ğŸ” æ£€ç´¢æ‰§è¡Œå±‚  
    â”œâ”€â”€ å¹¶è¡Œå‘é‡æœç´¢
    â”œâ”€â”€ å¹¶è¡Œå…¨æ–‡æœç´¢
    â”œâ”€â”€ RRFç»“æœèåˆ
    â””â”€â”€ å¤šç»´åº¦é‡æ’åº
          â†“
ğŸ¯ ç­”æ¡ˆç”Ÿæˆå±‚
    â”œâ”€â”€ ä¸Šä¸‹æ–‡æ„å»ºä¸æ ¼å¼åŒ–
    â”œâ”€â”€ æç¤ºå·¥ç¨‹ä¼˜åŒ–
    â”œâ”€â”€ LLMç”Ÿæˆè°ƒç”¨
    â””â”€â”€ ç»“æœåå¤„ç†
          â†“
ğŸ“¤ å“åº”è¾“å‡ºå±‚
    â”œâ”€â”€ ç»“æ„åŒ–JSONå“åº”
    â”œâ”€â”€ æ€§èƒ½ç»Ÿè®¡è®°å½•
    â””â”€â”€ ç¼“å­˜ç»“æœå­˜å‚¨
```

### 9.2 æ•°æ®æ‘„å–æµç¨‹

#### 9.2.1 æ‰¹é‡å‘é‡åŒ–ç®¡é“

```python
# ingest.py ä¸­çš„å®Œæ•´æµç¨‹
def main():
    """æ‰¹é‡æ•°æ®å¤„ç†ä¸»æµç¨‹"""
    
    # 1. æ•°æ®éªŒè¯ä¸åŠ è½½
    config.validate()
    db_manager.test_connection()
    df = load_properties_needing_vectorization()
    
    if len(df) == 0:
        print("ğŸ‰ æ‰€æœ‰æˆ¿æºæ•°æ®éƒ½å·²å®Œæˆå‘é‡åŒ–ï¼")
        return
    
    # 2. æ–‡æœ¬é¢„å¤„ç†ä¸æ ‡å‡†åŒ–
    texts = prepare_texts_for_vectorization(df)
    print(f"ğŸ“ å‡†å¤‡äº† {len(texts)} æ¡æ–‡æœ¬å†…å®¹ç”¨äºå‘é‡åŒ–")
    
    # 3. æ‰¹é‡å‘é‡åŒ–å¤„ç†
    embeddings = rag_service.generate_embeddings_batch(texts)
    print(f"âœ… æˆåŠŸç”Ÿæˆ {len(embeddings)} ä¸ªå‘é‡ï¼Œç»´åº¦: {len(embeddings[0])}")
    
    # 4. åŒé‡å­˜å‚¨æ›´æ–°
    update_database_with_vectors(df, embeddings)  # æ›´æ–°propertiesè¡¨
    add_to_vector_store(df)                       # æ›´æ–°LangChainå‘é‡å­˜å‚¨
    
    print("ğŸ‰ å‘é‡åŒ–å¤„ç†å®Œæˆï¼")
```
