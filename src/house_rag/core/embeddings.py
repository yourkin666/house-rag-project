"""
å‘é‡åŒ–å¤„ç†å’ŒRAGæ ¸å¿ƒé€»è¾‘æ¨¡å—
"""
import logging
import re
from typing import List, Dict, Any, Optional, Tuple
from functools import lru_cache
from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI
from langchain_community.vectorstores import PGVector
from langchain_core.documents import Document
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

from .config import config
from .database import db_manager

logger = logging.getLogger(__name__)


class HybridSearchResult:
    """æ··åˆæœç´¢ç»“æœç±»"""
    
    def __init__(self, property_id: int, vector_score: float = 0.0, fulltext_score: float = 0.0, 
                 vector_rank: int = 0, fulltext_rank: int = 0, final_score: float = 0.0):
        self.property_id = property_id
        self.vector_score = vector_score
        self.fulltext_score = fulltext_score
        self.vector_rank = vector_rank
        self.fulltext_rank = fulltext_rank
        self.final_score = final_score
        
    def __repr__(self):
        return (f"HybridSearchResult(id={self.property_id}, "
                f"vector_score={self.vector_score:.3f}, "
                f"fulltext_score={self.fulltext_score:.3f}, "
                f"final_score={self.final_score:.3f})")


class ReciprocalRankFusion:
    """
    Reciprocal Rank Fusion (RRF) ç®—æ³•å®ç°
    
    RRFæ˜¯ä¸€ç§æ— ç›‘ç£çš„æ’åèåˆæ–¹æ³•ï¼Œç‰¹åˆ«é€‚åˆç»“åˆä¸åŒæœç´¢ç³»ç»Ÿçš„ç»“æœã€‚
    å®ƒä¸éœ€è¦é¢„å…ˆçŸ¥é“å„ä¸ªæœç´¢ç³»ç»Ÿçš„å‡†ç¡®æ€§ï¼Œèƒ½å¤Ÿè‡ªåŠ¨å¹³è¡¡ä¸åŒç³»ç»Ÿçš„è´¡çŒ®ã€‚
    
    å…¬å¼: RRF_score(d) = Î£ 1/(k + rank_i(d))
    å…¶ä¸­ d æ˜¯æ–‡æ¡£ï¼Œrank_i(d) æ˜¯æ–‡æ¡£ d åœ¨ç¬¬ i ä¸ªæ’ååˆ—è¡¨ä¸­çš„ä½ç½®ï¼Œk æ˜¯å¹³æ»‘å‚æ•°
    """
    
    def __init__(self, k: int = 60):
        """
        åˆå§‹åŒ– RRF èåˆå™¨
        
        Args:
            k: RRF å¹³æ»‘å‚æ•°ï¼Œé»˜è®¤60ã€‚è¾ƒå¤§çš„kå€¼ä¼šå‡å°‘é«˜æ’åå’Œä½æ’åä¹‹é—´çš„å·®å¼‚
        """
        self.k = k
    
    def fuse_rankings(self, vector_results: List[Tuple[int, float]], 
                     fulltext_results: List[Tuple[int, float]], 
                     max_results: int = 50) -> List[HybridSearchResult]:
        """
        ä½¿ç”¨ RRF ç®—æ³•èåˆå‘é‡æœç´¢å’Œå…¨æ–‡æœç´¢çš„ç»“æœ
        
        Args:
            vector_results: å‘é‡æœç´¢ç»“æœ [(property_id, score), ...]
            fulltext_results: å…¨æ–‡æœç´¢ç»“æœ [(property_id, score), ...]
            max_results: è¿”å›çš„æœ€å¤§ç»“æœæ•°
            
        Returns:
            List[HybridSearchResult]: èåˆåçš„æ’åºç»“æœ
        """
        # æ”¶é›†æ‰€æœ‰å”¯ä¸€çš„property_id
        all_property_ids = set()
        vector_dict = {}
        fulltext_dict = {}
        
        # å¤„ç†å‘é‡æœç´¢ç»“æœ
        for rank, (prop_id, score) in enumerate(vector_results, 1):
            all_property_ids.add(prop_id)
            vector_dict[prop_id] = {'score': score, 'rank': rank}
        
        # å¤„ç†å…¨æ–‡æœç´¢ç»“æœ
        for rank, (prop_id, score) in enumerate(fulltext_results, 1):
            all_property_ids.add(prop_id)
            fulltext_dict[prop_id] = {'score': score, 'rank': rank}
        
        # è®¡ç®—èåˆåˆ†æ•°
        hybrid_results = []
        for prop_id in all_property_ids:
            vector_info = vector_dict.get(prop_id, {'score': 0.0, 'rank': len(vector_results) + 1})
            fulltext_info = fulltext_dict.get(prop_id, {'score': 0.0, 'rank': len(fulltext_results) + 1})
            
            # RRF åˆ†æ•°è®¡ç®—
            rrf_score = 0.0
            if prop_id in vector_dict:
                rrf_score += 1.0 / (self.k + vector_info['rank'])
            if prop_id in fulltext_dict:
                rrf_score += 1.0 / (self.k + fulltext_info['rank'])
            
            hybrid_result = HybridSearchResult(
                property_id=prop_id,
                vector_score=vector_info['score'],
                fulltext_score=fulltext_info['score'],
                vector_rank=vector_info['rank'],
                fulltext_rank=fulltext_info['rank'],
                final_score=rrf_score
            )
            hybrid_results.append(hybrid_result)
        
        # æŒ‰èåˆåˆ†æ•°æ’åº
        hybrid_results.sort(key=lambda x: x.final_score, reverse=True)
        
        return hybrid_results[:max_results]


class RAGService:
    """RAGæœåŠ¡ç±»ï¼Œå¤„ç†å‘é‡åŒ–å’Œé—®ç­”"""
    
    def __init__(self):
        self.embeddings = None
        self.llm = None
        self.vector_store = None
        self.rag_chain = None
        self._query_cache = {}  # ç®€å•çš„æŸ¥è¯¢ç¼“å­˜
        self._max_cache_size = 100  # ç¼“å­˜å¤§å°é™åˆ¶
        
        # æˆæœ¬æ§åˆ¶å’Œç»Ÿè®¡
        self._intent_cache = {}  # æ„å›¾åˆ†æç¼“å­˜
        self._llm_call_stats = {
            'total_calls': 0,
            'cache_hits': 0,
            'keyword_fallbacks': 0,
            'hourly_calls': 0,
            'last_reset_time': None
        }
        
        # æŸ¥è¯¢ç»Ÿè®¡
        self._query_stats = {
            'total_queries': 0,
            'cache_hit_queries': 0,
            'avg_results_per_query': 0,
            'total_results_returned': 0,
            'last_stats_log': None
        }
        
        # æ··åˆæœç´¢ç›¸å…³ç»„ä»¶ - ä¼˜åŒ–RRFå‚æ•°
        self.rrf_fusion = ReciprocalRankFusion(k=40)  # ä»60è°ƒæ•´åˆ°40ï¼Œå¢å¼ºé«˜æ’åå·®å¼‚
        self.hybrid_search_enabled = True  # æ§åˆ¶æ˜¯å¦å¯ç”¨æ··åˆæœç´¢
        self.hybrid_search_stats = {
            'total_hybrid_searches': 0,
            'vector_only_fallbacks': 0,
            'fulltext_contributions': 0
        }
        
        self._initialize()
    
    def _initialize(self) -> None:
        """åˆå§‹åŒ–RAGç»„ä»¶"""
        try:
            # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
            self.embeddings = GoogleGenerativeAIEmbeddings(
                model="models/text-embedding-004",
                google_api_key=config.GOOGLE_API_KEY
            )
            
            # åˆå§‹åŒ–LLM
            self.llm = ChatGoogleGenerativeAI(
                model="gemini-1.5-flash",
                google_api_key=config.GOOGLE_API_KEY,
                temperature=0.1,
                max_tokens=2048
            )
            
            # åˆå§‹åŒ–å‘é‡å­˜å‚¨
            self.vector_store = PGVector(
                connection_string=config.pgvector_connection_string,
                embedding_function=self.embeddings,
                collection_name=config.COLLECTION_NAME,
                distance_strategy="cosine"
            )
            
            # åˆ›å»ºRAGé“¾
            self._create_rag_chain()
            
            logger.info("RAGæœåŠ¡åˆå§‹åŒ–å®Œæˆ")
            
        except Exception as e:
            logger.error(f"RAGæœåŠ¡åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    def _assess_extraction_quality(self, params: Dict[str, Any], question: str) -> Dict[str, Any]:
        """è¯„ä¼°è§„åˆ™æå–ç»“æœçš„è´¨é‡ï¼Œå†³å®šæ˜¯å¦éœ€è¦LLMåå¤‡å¤„ç†"""
        quality_score = 0
        max_score = 5
        extraction_info = {
            'quality_score': 0,
            'needs_llm_fallback': False,
            'reasons': [],
            'extracted_fields_count': 0
        }
        
        # ç»Ÿè®¡æˆåŠŸæå–çš„å­—æ®µæ•°é‡
        extracted_fields = 0
        if params['price_range'] is not None:
            extracted_fields += 1
            quality_score += 1
        if params['location_keywords']:
            extracted_fields += 1
            quality_score += 1
        if params['property_type'] is not None:
            extracted_fields += 1
            quality_score += 1
        if params['area_preference'] is not None:
            extracted_fields += 1
            quality_score += 1
        if params['special_requirements']:
            extracted_fields += 1
            quality_score += 1
        
        extraction_info['extracted_fields_count'] = extracted_fields
        extraction_info['quality_score'] = quality_score
        
        # å†³å®šæ˜¯å¦éœ€è¦LLMåå¤‡å¤„ç†çš„é€»è¾‘
        question_length = len(question)
        
        # æƒ…å†µ1: å®Œå…¨æ²¡æœ‰æå–åˆ°ä»»ä½•ä¿¡æ¯
        if extracted_fields == 0:
            extraction_info['needs_llm_fallback'] = True
            extraction_info['reasons'].append("è§„åˆ™æå–æœªæ‰¾åˆ°ä»»ä½•å‚æ•°")
        
        # æƒ…å†µ2: é—®é¢˜å¾ˆé•¿ä½†æå–ä¿¡æ¯å¾ˆå°‘ï¼ˆå¯èƒ½åŒ…å«å¤æ‚è¡¨è¾¾ï¼‰
        elif question_length > 30 and extracted_fields <= 1:
            extraction_info['needs_llm_fallback'] = True
            extraction_info['reasons'].append("å¤æ‚æŸ¥è¯¢ä½†è§„åˆ™æå–ä¿¡æ¯ä¸è¶³")
        
        # æƒ…å†µ3: åŒ…å«ä¸€äº›å¤æ‚çš„è¯­è¨€æ¨¡å¼ï¼Œè§„åˆ™å¯èƒ½æ— æ³•å¤„ç†
        complex_patterns = [
            'è¦ä¹ˆ', 'æˆ–è€…', 'ä¸è¿‡', 'ä½†æ˜¯', 'é™¤äº†', 'å¦å¤–',
            'æœ€å¥½æ˜¯', 'å¸Œæœ›', 'æ¯”è¾ƒ', 'ç›¸å¯¹', 'å¤§æ¦‚', 'å·®ä¸å¤š',
            'å¦‚æœ', 'å‡å¦‚', 'å€˜è‹¥'
        ]
        if any(pattern in question for pattern in complex_patterns):
            if extracted_fields <= 2:  # å¤æ‚è¡¨è¾¾ä½†ä¿¡æ¯æå–å°‘
                extraction_info['needs_llm_fallback'] = True
                extraction_info['reasons'].append("æ£€æµ‹åˆ°å¤æ‚è¯­è¨€æ¨¡å¼")
        
        # æƒ…å†µ4: è´¨é‡å¾—åˆ†è¿‡ä½
        if quality_score < 2 and question_length > 15:
            extraction_info['needs_llm_fallback'] = True
            extraction_info['reasons'].append("æ•´ä½“æå–è´¨é‡åä½")
        
        logger.info(f"è§„åˆ™æå–è´¨é‡è¯„ä¼°: å¾—åˆ†{quality_score}/{max_score}, "
                   f"æå–å­—æ®µ{extracted_fields}ä¸ª, éœ€è¦LLMåå¤‡: {extraction_info['needs_llm_fallback']}")
        
        return extraction_info

    def _extract_parameters_with_llm(self, question: str) -> Dict[str, Any]:
        """ä½¿ç”¨LLMè¿›è¡Œç»“æ„åŒ–å‚æ•°æå–ï¼ˆåå¤‡æ–¹æ¡ˆï¼‰"""
        try:
            # æ„å»ºä¸“é—¨çš„æå–æŒ‡ä»¤
            extraction_prompt = f"""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æˆ¿äº§æŸ¥è¯¢å‚æ•°æå–åŠ©æ‰‹ã€‚è¯·ä»ç”¨æˆ·çš„æ‰¾æˆ¿é—®é¢˜ä¸­æå–å‡ºä»¥ä¸‹ç»“æ„åŒ–ä¿¡æ¯ï¼Œå¹¶ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¿”å›ï¼š

ç”¨æˆ·é—®é¢˜ï¼š"{question}"

è¯·æå–ä»¥ä¸‹ä¿¡æ¯ï¼ˆå¦‚æœæŸé¡¹ä¿¡æ¯æœªæåŠæˆ–æ— æ³•ç¡®å®šï¼Œè¯·è®¾ä¸ºnullï¼‰ï¼š

1. price_range: ä»·æ ¼åŒºé—´ï¼Œæ ¼å¼ä¸º[æœ€å°å€¼, æœ€å¤§å€¼]ï¼Œå•ä½ä¸‡å…ƒã€‚å¦‚æœåªè¯´äº†ä¸Šé™ï¼Œæœ€å°å€¼è®¾ä¸º0
2. location_keywords: åœ°ç†ä½ç½®å…³é”®è¯åˆ—è¡¨ï¼Œæå–æ‰€æœ‰æåŠçš„åœ°åã€åŒºåŸŸç­‰
3. property_type: æˆ¿å±‹ç±»å‹ï¼Œå¯é€‰å€¼ï¼š"å…¬å¯“"ã€"ä½å®…"ã€"åˆ«å¢…"ã€"æ´‹æˆ¿"ï¼Œåªèƒ½é€‰ä¸€ä¸ª
4. area_preference: é¢ç§¯åå¥½ï¼Œæå–æ•°å­—ï¼Œå•ä½å¹³æ–¹ç±³
5. special_requirements: ç‰¹æ®Šéœ€æ±‚åˆ—è¡¨ï¼Œå¦‚å­¦åŒºã€åœ°é“ã€åœè½¦ã€ç”µæ¢¯ç­‰

è¯·åªè¿”å›JSONæ ¼å¼çš„ç»“æœï¼Œä¸è¦åŒ…å«å…¶ä»–è¯´æ˜æ–‡å­—ï¼š
```json
{{
  "price_range": null æˆ– [æ•°å­—1, æ•°å­—2],
  "location_keywords": [å­—ç¬¦ä¸²åˆ—è¡¨],
  "property_type": null æˆ– "ç±»å‹å­—ç¬¦ä¸²", 
  "area_preference": null æˆ– æ•°å­—,
  "special_requirements": [å­—ç¬¦ä¸²åˆ—è¡¨]
}}
```"""

            # è°ƒç”¨LLMè¿›è¡Œæå–
            response = self.llm.invoke(extraction_prompt)
            response_text = response.content.strip()
            
            # å°è¯•ä»å“åº”ä¸­æå–JSON
            import json
            import re
            
            # æŸ¥æ‰¾JSONå—
            json_match = re.search(r'```json\s*(.*?)\s*```', response_text, re.DOTALL)
            if json_match:
                json_str = json_match.group(1)
            else:
                # å¦‚æœæ²¡æœ‰ä»£ç å—ï¼Œå°è¯•ç›´æ¥è§£ææ•´ä¸ªå“åº”
                json_str = response_text
            
            # è§£æJSON
            extracted_params = json.loads(json_str)
            
            # æ•°æ®æ¸…æ´—å’ŒéªŒè¯
            cleaned_params = {
                'price_range': None,
                'location_keywords': [],
                'property_type': None,
                'area_preference': None,
                'special_requirements': []
            }
            
            # å¤„ç†ä»·æ ¼èŒƒå›´
            if extracted_params.get('price_range'):
                price_range = extracted_params['price_range']
                if isinstance(price_range, list) and len(price_range) == 2:
                    cleaned_params['price_range'] = tuple(price_range)
            
            # å¤„ç†åœ°ç†ä½ç½®
            if extracted_params.get('location_keywords'):
                if isinstance(extracted_params['location_keywords'], list):
                    cleaned_params['location_keywords'] = extracted_params['location_keywords']
            
            # å¤„ç†æˆ¿å±‹ç±»å‹
            if extracted_params.get('property_type'):
                valid_types = ['å…¬å¯“', 'ä½å®…', 'åˆ«å¢…', 'æ´‹æˆ¿']
                if extracted_params['property_type'] in valid_types:
                    cleaned_params['property_type'] = extracted_params['property_type']
            
            # å¤„ç†é¢ç§¯åå¥½
            if extracted_params.get('area_preference'):
                if isinstance(extracted_params['area_preference'], (int, float)):
                    cleaned_params['area_preference'] = int(extracted_params['area_preference'])
            
            # å¤„ç†ç‰¹æ®Šéœ€æ±‚
            if extracted_params.get('special_requirements'):
                if isinstance(extracted_params['special_requirements'], list):
                    cleaned_params['special_requirements'] = extracted_params['special_requirements']
            
            logger.info(f"LLMæå–æˆåŠŸ: {cleaned_params}")
            return cleaned_params
            
        except Exception as e:
            logger.error(f"LLMå‚æ•°æå–å¤±è´¥: {e}")
            # è¿”å›ç©ºå‚æ•°ä½œä¸ºé™çº§æ–¹æ¡ˆ
            return {
                'price_range': None,
                'location_keywords': [],
                'property_type': None,
                'area_preference': None,
                'special_requirements': []
            }

    def _extract_search_parameters_rule_based(self, question: str) -> Dict[str, Any]:
        """åŸºäºè§„åˆ™çš„å‚æ•°æå–ï¼ˆåŸæœ‰é€»è¾‘ï¼‰"""
        params = {
            'price_range': None,
            'location_keywords': [],
            'property_type': None,
            'area_preference': None,
            'special_requirements': []
        }
        
        # æå–ä»·æ ¼èŒƒå›´
        price_patterns = [
            r'(\d+)(?:ä¸‡)?[-åˆ°](\d+)ä¸‡',
            r'(\d+)-(\d+)ä¸‡',
            r'(\d+)ä¸‡ä»¥å†…',
            r'ä¸è¶…è¿‡(\d+)ä¸‡',
            r'é¢„ç®—(\d+)ä¸‡?å·¦å³',
            r'(\d+)ä¸‡å·¦å³'
        ]
        
        for pattern in price_patterns:
            match = re.search(pattern, question)
            if match:
                if len(match.groups()) == 2:
                    params['price_range'] = (int(match.group(1)), int(match.group(2)))
                else:
                    params['price_range'] = (0, int(match.group(1)))
                break
        
        # æå–åœ°ç†ä½ç½®å…³é”®è¯
        location_markers = ['åŒº', 'è·¯', 'è¡—', 'é•‡', 'å¸‚', 'å¿', 'æ–°åŒº', 'å¼€å‘åŒº', 'é™„è¿‘', 'å‘¨è¾¹']
        words = re.findall(r'[\u4e00-\u9fff]+', question)  # æå–ä¸­æ–‡è¯æ±‡
        
        for word in words:
            if any(marker in word for marker in location_markers):
                params['location_keywords'].append(word)
        
        # æå–æˆ¿å±‹ç±»å‹
        if any(keyword in question for keyword in ['å…¬å¯“', 'ä½å®…', 'åˆ«å¢…', 'æ´‹æˆ¿']):
            for prop_type in ['å…¬å¯“', 'ä½å®…', 'åˆ«å¢…', 'æ´‹æˆ¿']:
                if prop_type in question:
                    params['property_type'] = prop_type
                    break
        
        # æå–é¢ç§¯åå¥½
        area_match = re.search(r'(\d+)(?:å¹³|å¹³æ–¹|ã¡)', question)
        if area_match:
            params['area_preference'] = int(area_match.group(1))
        
        # æå–ç‰¹æ®Šéœ€æ±‚
        special_keywords = ['å­¦åŒº', 'åœ°é“', 'å•†åœˆ', 'åŒ»é™¢', 'å…¬å›­', 'åœè½¦', 'ç”µæ¢¯', 'æœå—']
        for keyword in special_keywords:
            if keyword in question:
                params['special_requirements'].append(keyword)
        
        return params

    def _extract_search_parameters(self, question: str) -> Dict[str, Any]:
        """
        æ··åˆæ¨¡å¼å‚æ•°æå–ï¼šé¦–å…ˆä½¿ç”¨è§„åˆ™æå–ï¼Œå¿…è¦æ—¶ä½¿ç”¨LLMåå¤‡
        """
        # ç¬¬ä¸€é˜¶æ®µï¼šè§„åˆ™æå–
        rule_based_params = self._extract_search_parameters_rule_based(question)
        
        # ç¬¬äºŒé˜¶æ®µï¼šè´¨é‡è¯„ä¼°
        quality_info = self._assess_extraction_quality(rule_based_params, question)
        
        # ç¬¬ä¸‰é˜¶æ®µï¼šå†³å®šæ˜¯å¦éœ€è¦LLMåå¤‡
        if quality_info['needs_llm_fallback']:
            logger.info(f"è§¦å‘LLMåå¤‡æå–ï¼ŒåŸå› : {', '.join(quality_info['reasons'])}")
            
            # ä½¿ç”¨LLMæå–
            llm_params = self._extract_parameters_with_llm(question)
            
            # åˆå¹¶è§„åˆ™æå–å’ŒLLMæå–ç»“æœ
            merged_params = self._merge_extraction_results(rule_based_params, llm_params, question)
            
            # æ·»åŠ æå–å…ƒæ•°æ®
            merged_params['_extraction_metadata'] = {
                'method': 'hybrid',
                'rule_quality': quality_info,
                'used_llm_fallback': True,
                'fallback_reasons': quality_info['reasons']
            }
            
            return merged_params
        
        else:
            logger.info("è§„åˆ™æå–è´¨é‡è‰¯å¥½ï¼Œä½¿ç”¨è§„åˆ™æå–ç»“æœ")
            # æ·»åŠ æå–å…ƒæ•°æ®
            rule_based_params['_extraction_metadata'] = {
                'method': 'rule_based_only',
                'rule_quality': quality_info,
                'used_llm_fallback': False
            }
            
            return rule_based_params

    def _merge_extraction_results(self, rule_params: Dict[str, Any], llm_params: Dict[str, Any], question: str) -> Dict[str, Any]:
        """
        æ™ºèƒ½åˆå¹¶è§„åˆ™æå–å’ŒLLMæå–çš„ç»“æœ
        ç­–ç•¥ï¼šä¼˜å…ˆä½¿ç”¨è§„åˆ™ç»“æœï¼ˆæ›´ç²¾ç¡®ï¼‰ï¼ŒLLMç»“æœä½œä¸ºè¡¥å……
        """
        merged = {
            'price_range': None,
            'location_keywords': [],
            'property_type': None,
            'area_preference': None,
            'special_requirements': []
        }
        
        # ä»·æ ¼èŒƒå›´ï¼šä¼˜å…ˆä½¿ç”¨è§„åˆ™ç»“æœï¼Œå› ä¸ºè§„åˆ™å¯¹æ•°å­—å¤„ç†æ›´å‡†ç¡®
        merged['price_range'] = rule_params['price_range'] or llm_params['price_range']
        
        # åœ°ç†ä½ç½®ï¼šåˆå¹¶ä¸¤ä¸ªç»“æœï¼Œå»é‡
        all_locations = set(rule_params['location_keywords'] + llm_params['location_keywords'])
        merged['location_keywords'] = list(all_locations)
        
        # æˆ¿å±‹ç±»å‹ï¼šä¼˜å…ˆä½¿ç”¨è§„åˆ™ç»“æœ
        merged['property_type'] = rule_params['property_type'] or llm_params['property_type']
        
        # é¢ç§¯åå¥½ï¼šä¼˜å…ˆä½¿ç”¨è§„åˆ™ç»“æœ
        merged['area_preference'] = rule_params['area_preference'] or llm_params['area_preference']
        
        # ç‰¹æ®Šéœ€æ±‚ï¼šåˆå¹¶ä¸¤ä¸ªç»“æœï¼Œå»é‡
        all_requirements = set(rule_params['special_requirements'] + llm_params['special_requirements'])
        merged['special_requirements'] = list(all_requirements)
        
        logger.info(f"åˆå¹¶æå–ç»“æœå®Œæˆ: è§„åˆ™å­—æ®µ{self._count_extracted_fields(rule_params)}ä¸ª, "
                   f"LLMå­—æ®µ{self._count_extracted_fields(llm_params)}ä¸ª, "
                   f"æœ€ç»ˆå­—æ®µ{self._count_extracted_fields(merged)}ä¸ª")
        
        return merged

    def _count_extracted_fields(self, params: Dict[str, Any]) -> int:
        """è®¡ç®—æå–åˆ°çš„æœ‰æ•ˆå­—æ®µæ•°é‡"""
        count = 0
        if params.get('price_range'):
            count += 1
        if params.get('location_keywords'):
            count += 1
        if params.get('property_type'):
            count += 1
        if params.get('area_preference'):
            count += 1
        if params.get('special_requirements'):
            count += 1
        return count

    def _clean_params_for_processing(self, params: Dict[str, Any]) -> Dict[str, Any]:
        """ç§»é™¤å‚æ•°ä¸­çš„å…ƒæ•°æ®ï¼Œè¿”å›å¹²å‡€çš„å¤„ç†å‚æ•°"""
        clean_params = {
            'price_range': params.get('price_range'),
            'location_keywords': params.get('location_keywords', []),
            'property_type': params.get('property_type'),
            'area_preference': params.get('area_preference'),
            'special_requirements': params.get('special_requirements', [])
        }
        return clean_params
    
    def _calculate_dynamic_k(self, search_params: Dict[str, Any], question: str, base_k: int = 5, max_k: int = 12, min_k: int = 4) -> int:
        """
        æ ¹æ®æŸ¥è¯¢å¤æ‚åº¦åŠ¨æ€è°ƒæ•´æ£€ç´¢æ•°é‡ (ä¼˜åŒ–ç‰ˆ)
        ä¸»è¦ä¾æ®æå–å‡ºçš„ç»“æ„åŒ–å‚æ•°æ•°é‡æ¥è¯„ä¼°å¤æ‚åº¦ï¼Œæ¯”å•çº¯çš„å­—ç¬¦ä¸²åˆ†ææ›´å‡†ç¡®
        """
        complexity_score = 0
        
        # 1. åŸºäºæå–å‡ºçš„å‚æ•°æ•°é‡ï¼ˆæœ€å¯é çš„æŒ‡æ ‡ï¼‰
        # æ¯ä¸ªæœ‰æ•ˆçš„æœç´¢ç»´åº¦éƒ½å¢åŠ å¤æ‚åº¦
        if search_params.get('price_range'):
            complexity_score += 1
        if search_params.get('location_keywords'):
            complexity_score += len(search_params['location_keywords'])  # å¤šä¸ªåœ°ç‚¹å¢åŠ æ›´å¤šå¤æ‚åº¦
        if search_params.get('property_type'):
            complexity_score += 1
        if search_params.get('area_preference'):
            complexity_score += 1
        # ç‰¹æ®Šéœ€æ±‚è¶Šå¤šï¼Œå¤æ‚åº¦è¶Šé«˜
        if search_params.get('special_requirements'):
            complexity_score += len(search_params['special_requirements'])
        
        # 2. åŸºäºé—®é¢˜ä¸­çš„é€»è¾‘è¿æ¥è¯ä½œä¸ºè¡¥å……ï¼ˆå¤„ç†å¤æ‚é€»è¾‘å…³ç³»ï¼‰
        logical_keywords = ['å¹¶ä¸”', 'åŒæ—¶', 'æˆ–è€…', 'è¦ä¹ˆ', 'å¦å¤–', 'è€Œä¸”', 'ä»¥åŠ']
        logical_complexity = sum(1 for keyword in logical_keywords if keyword in question)
        complexity_score += logical_complexity
        
        # 3. åŸºäºé—®é¢˜é•¿åº¦ä½œä¸ºå¾®è°ƒï¼ˆé•¿é—®é¢˜é€šå¸¸åŒ…å«æ›´å¤šç»†èŠ‚ï¼‰
        query_length = len(question)
        if query_length > 80:
            complexity_score += 2
        elif query_length > 50:
            complexity_score += 1
            
        # 4. æ£€æµ‹æ¨¡ç³ŠæŸ¥è¯¢ï¼ˆéœ€è¦æ›´å¤šç»“æœæ¥æ»¡è¶³ç”¨æˆ·æœŸæœ›ï¼‰
        vague_indicators = ['æ¨è', 'æœ‰ä»€ä¹ˆ', 'çœ‹çœ‹', 'æ‰¾æ‰¾', 'åˆé€‚çš„']
        if any(indicator in question for indicator in vague_indicators):
            complexity_score += 1
        
        # åŠ¨æ€è°ƒæ•´kå€¼ï¼Œç¡®ä¿åœ¨åˆç†èŒƒå›´å†…
        adjusted_k = min(base_k + complexity_score, max_k)
        final_k = max(adjusted_k, min_k)
        
        # è®°å½•å¤æ‚åº¦åˆ†ææ—¥å¿—ï¼Œä¾¿äºè°ƒè¯•å’Œä¼˜åŒ–
        logger.debug(f"å¤æ‚åº¦åˆ†æ - å‚æ•°ç»´åº¦: {self._count_extracted_fields(search_params)}, "
                    f"é€»è¾‘è¯: {logical_complexity}, é•¿åº¦: {query_length}, "
                    f"æœ€ç»ˆå¤æ‚åº¦: {complexity_score}, Kå€¼: {final_k}")
        
        return final_k
    
    def _add_to_cache(self, cache_dict: dict, key: str, value: any) -> None:
        """ç®€å•çš„ç¼“å­˜ç®¡ç†ï¼Œé˜²æ­¢å†…å­˜æ³„æ¼"""
        if len(cache_dict) >= self._max_cache_size:
            # ç®€å•ç²—æš´ï¼šåˆ é™¤ä¸€åŠæ—§ç¼“å­˜ï¼Œé¿å…å¤æ‚çš„LRUå®ç°
            keys_to_delete = list(cache_dict.keys())[:self._max_cache_size//2]
            for k in keys_to_delete:
                del cache_dict[k]
            logger.info(f"ç¼“å­˜å·²æ¸…ç†ï¼Œåˆ é™¤äº†{len(keys_to_delete)}ä¸ªæ—§æ¡ç›®")
        cache_dict[key] = value
    
    def _get_adaptive_retriever_config(self, question: str, dynamic_k: int) -> dict:
        """æ ¹æ®æŸ¥è¯¢ç±»å‹è¿”å›æœ€ä½³çš„æ£€ç´¢å™¨é…ç½®ï¼ˆæˆæœ¬ä¼˜åŒ–ç‰ˆï¼‰"""
        
        # é¦–å…ˆæ£€æŸ¥ç¼“å­˜
        cache_key = f"intent_{hash(question)}"
        if config.ENABLE_INTENT_CACHE and cache_key in self._intent_cache:
            logger.debug(f"ä½¿ç”¨ç¼“å­˜çš„æ„å›¾åˆ†æç»“æœ: {question[:30]}...")
            self._llm_call_stats['cache_hits'] += 1
            return self._build_strategy_from_intents(self._intent_cache[cache_key], dynamic_k)
        
        # æˆæœ¬ä¼˜åŒ–ï¼šæ··åˆç­–ç•¥ - å…ˆåˆ¤æ–­æ˜¯å¦éœ€è¦LLM
        if self._should_use_llm_analysis(question):
            try:
                # åªå¯¹å¤æ‚æŸ¥è¯¢ä½¿ç”¨LLM
                intents = self._classify_intent_with_llm(question)
                logger.info(f"LLMæ„å›¾åˆ†æ - æŸ¥è¯¢: {question[:30]}..., ç»“æœ: {intents}")
                
                # æ›´æ–°ç»Ÿè®¡
                self._llm_call_stats['total_calls'] += 1
                self._llm_call_stats['hourly_calls'] += 1
                
                # ç¼“å­˜ç»“æœ
                if config.ENABLE_INTENT_CACHE:
                    self._add_to_cache(self._intent_cache, cache_key, intents)
                
                return self._build_strategy_from_intents(intents, dynamic_k)
                
            except Exception as e:
                logger.warning(f"LLMæ„å›¾åˆ†æå¤±è´¥ï¼Œå›é€€åˆ°å…³é”®è¯åŒ¹é…: {e}")
                return self._get_keyword_based_config(question, dynamic_k)
        else:
            # ç®€å•æŸ¥è¯¢ç›´æ¥ä½¿ç”¨å…³é”®è¯åŒ¹é…ï¼ŒèŠ‚çœæˆæœ¬
            logger.debug(f"ç®€å•æŸ¥è¯¢ä½¿ç”¨å…³é”®è¯åŒ¹é…: {question[:30]}...")
            return self._get_keyword_based_config(question, dynamic_k)
    
    def _should_use_llm_analysis(self, question: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦éœ€è¦ä½¿ç”¨LLMè¿›è¡Œæ„å›¾åˆ†æï¼ˆæˆæœ¬æ§åˆ¶ï¼‰"""
        
        # æ£€æŸ¥å¹¶é‡ç½®å°æ—¶è®¡æ•°å™¨
        self._reset_hourly_counter_if_needed()
        
        # æˆæœ¬æ§åˆ¶ï¼šæ£€æŸ¥LLMè°ƒç”¨é¢‘ç‡é™åˆ¶
        if self._llm_call_stats['hourly_calls'] >= config.MAX_LLM_CALLS_PER_HOUR:
            logger.warning(f"LLMè°ƒç”¨å·²è¾¾å°æ—¶é™åˆ¶({config.MAX_LLM_CALLS_PER_HOUR})ï¼Œä½¿ç”¨å…³é”®è¯åŒ¹é…")
            self._llm_call_stats['keyword_fallbacks'] += 1
            return False
        
        # ç®€å•æŸ¥è¯¢æ¡ä»¶ï¼šç›´æ¥ç”¨å…³é”®è¯åŒ¹é…å°±è¶³å¤Ÿ
        simple_conditions = [
            len(question) < 15,  # å¾ˆçŸ­çš„æŸ¥è¯¢
            any(simple in question for simple in ['æ¨è', 'æœ‰ä»€ä¹ˆ', 'çœ‹çœ‹']),  # çº¯æ¢ç´¢æ€§
            bool(re.search(r'^\w+æˆ¿$', question)),  # å¦‚"å­¦åŒºæˆ¿"ã€"äºŒæ‰‹æˆ¿"
        ]
        
        if any(simple_conditions):
            logger.debug(f"ç®€å•æŸ¥è¯¢ï¼Œç›´æ¥ä½¿ç”¨å…³é”®è¯: {question[:30]}...")
            return False
            
        # å¤æ‚æŸ¥è¯¢æ¡ä»¶ï¼šéœ€è¦LLMæ·±åº¦ç†è§£
        complex_conditions = [
            'ä¸è¦' in question or 'åˆ«' in question,  # åŒ…å«å¦å®šè¯
            len(re.findall(r'[ï¼Œ,]', question)) >= 2,  # å¤šä¸ªæ¡ä»¶ç”¨é€—å·åˆ†éš”
            any(compound in question for compound in ['è€Œä¸”', 'ä½†æ˜¯', 'ä¸è¿‡', 'åŒæ—¶']),  # å¤åˆé€»è¾‘
            len([w for w in ['ä»·æ ¼', 'ä½ç½®', 'æˆ¿å‹', 'é¢ç§¯', 'å­¦åŒº', 'åœ°é“'] if w in question]) >= 2,  # å¤šç»´åº¦éœ€æ±‚
            'æ€§ä»·æ¯”' in question and any(special in question for special in ['å­¦åŒº', 'åœ°é“', 'æ™¯è§‚']),  # æ˜æ˜¾çš„å¤åˆéœ€æ±‚
        ]
        
        # å¦‚æœæ»¡è¶³å¤æ‚æ¡ä»¶ï¼Œä½¿ç”¨LLM
        if any(complex_conditions):
            logger.debug(f"æ£€æµ‹åˆ°å¤æ‚æŸ¥è¯¢ï¼Œä½¿ç”¨LLMåˆ†æ: {question[:50]}...")
            return True
            
        # ä¸­ç­‰å¤æ‚åº¦æŸ¥è¯¢ï¼šéšæœºé‡‡æ ·ï¼Œå¹³è¡¡æˆæœ¬å’Œæ•ˆæœ
        import random
        use_llm = random.random() < config.LLM_SAMPLING_RATE
        if use_llm:
            logger.debug(f"ä¸­ç­‰å¤æ‚æŸ¥è¯¢éšæœºé€‰æ‹©LLMåˆ†æ(æ¦‚ç‡{config.LLM_SAMPLING_RATE}): {question[:50]}...")
        
        return use_llm
    
    def _reset_hourly_counter_if_needed(self):
        """é‡ç½®å°æ—¶è®¡æ•°å™¨ï¼ˆå¦‚æœéœ€è¦ï¼‰"""
        import datetime
        
        now = datetime.datetime.now()
        last_reset = self._llm_call_stats.get('last_reset_time')
        
        if last_reset is None or (now - last_reset).seconds >= 3600:  # è¶…è¿‡1å°æ—¶
            self._llm_call_stats['hourly_calls'] = 0
            self._llm_call_stats['last_reset_time'] = now
            logger.info("é‡ç½®LLMå°æ—¶è°ƒç”¨è®¡æ•°å™¨")
    
    def _update_cost_stats(self, llm_calls: int, results_count: int) -> None:
        """æ›´æ–°æˆæœ¬å’ŒæŸ¥è¯¢ç»Ÿè®¡"""
        self._query_stats['total_queries'] += 1
        self._query_stats['total_results_returned'] += results_count
        
        if self._query_stats['total_queries'] > 0:
            self._query_stats['avg_results_per_query'] = (
                self._query_stats['total_results_returned'] / self._query_stats['total_queries']
            )
        
        # æ¯10æ¬¡æŸ¥è¯¢è®°å½•ä¸€æ¬¡ç»Ÿè®¡æ—¥å¿—
        if self._query_stats['total_queries'] % 10 == 0:
            self._log_performance_stats()
    
    def _log_performance_stats(self) -> None:
        """è®°å½•æ€§èƒ½ç»Ÿè®¡æ—¥å¿—"""
        import datetime
        
        now = datetime.datetime.now()
        query_stats = self._query_stats
        llm_stats = self._llm_call_stats
        
        cache_hit_rate = (query_stats['cache_hit_queries'] / max(1, query_stats['total_queries'])) * 100
        llm_hit_rate = (llm_stats['cache_hits'] / max(1, llm_stats['total_calls'])) * 100
        
        logger.info(f"""
ğŸ“Š æ€§èƒ½ç»Ÿè®¡æŠ¥å‘Š ({now.strftime('%H:%M:%S')})
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ” æŸ¥è¯¢ç»Ÿè®¡:
   â€¢ æ€»æŸ¥è¯¢æ•°: {query_stats['total_queries']}
   â€¢ ç¼“å­˜å‘½ä¸­: {query_stats['cache_hit_queries']} ({cache_hit_rate:.1f}%)
   â€¢ å¹³å‡ç»“æœæ•°: {query_stats['avg_results_per_query']:.1f}
   
ğŸ’° æˆæœ¬ç»Ÿè®¡:
   â€¢ LLMæ€»è°ƒç”¨: {llm_stats['total_calls']}
   â€¢ æ„å›¾ç¼“å­˜å‘½ä¸­: {llm_stats['cache_hits']} ({llm_hit_rate:.1f}%)
   â€¢ å…³é”®è¯å›é€€: {llm_stats['keyword_fallbacks']}
   â€¢ æœ¬å°æ—¶è°ƒç”¨: {llm_stats['hourly_calls']}/{config.MAX_LLM_CALLS_PER_HOUR}
   
ğŸ—„ï¸ ç¼“å­˜çŠ¶æ€:
   â€¢ æŸ¥è¯¢ç¼“å­˜: {len(self._query_cache)}/{self._max_cache_size}
   â€¢ æ„å›¾ç¼“å­˜: {len(self._intent_cache)}/{self._max_cache_size}
   
ğŸ”§ æ··åˆæœç´¢:
   â€¢ æ€»æœç´¢: {self.hybrid_search_stats['total_hybrid_searches']}
   â€¢ å‘é‡å›é€€: {self.hybrid_search_stats['vector_only_fallbacks']}
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”""")
        
        self._query_stats['last_stats_log'] = now
    
    def get_cost_stats(self) -> dict:
        """è·å–æˆæœ¬ç»Ÿè®¡ä¿¡æ¯ï¼ˆå¢å¼ºç‰ˆï¼‰"""
        llm_stats = self._llm_call_stats.copy()
        query_stats = self._query_stats.copy()
        
        # è®¡ç®—æ•ˆç‡æŒ‡æ ‡
        cache_hit_rate = (query_stats['cache_hit_queries'] / max(1, query_stats['total_queries'])) * 100
        llm_cache_hit_rate = (llm_stats['cache_hits'] / max(1, llm_stats['total_calls'])) * 100
        
        return {
            # LLMæˆæœ¬ç»Ÿè®¡
            'llm_total_calls': llm_stats['total_calls'],
            'llm_cache_hits': llm_stats['cache_hits'],
            'llm_cache_hit_rate': round(llm_cache_hit_rate, 1),
            'llm_hourly_calls': llm_stats['hourly_calls'],
            'llm_hourly_limit': config.MAX_LLM_CALLS_PER_HOUR,
            'keyword_fallbacks': llm_stats['keyword_fallbacks'],
            
            # æŸ¥è¯¢ç»Ÿè®¡
            'total_queries': query_stats['total_queries'],
            'cache_hit_queries': query_stats['cache_hit_queries'],
            'query_cache_hit_rate': round(cache_hit_rate, 1),
            'avg_results_per_query': round(query_stats['avg_results_per_query'], 1),
            
            # ç¼“å­˜çŠ¶æ€
            'query_cache_size': len(self._query_cache),
            'intent_cache_size': len(self._intent_cache),
            'max_cache_size': self._max_cache_size,
            
            # æ··åˆæœç´¢ç»Ÿè®¡
            'hybrid_searches': self.hybrid_search_stats['total_hybrid_searches'],
            'vector_fallbacks': self.hybrid_search_stats['vector_only_fallbacks'],
            'fulltext_contributions': self.hybrid_search_stats['fulltext_contributions'],
            
            # æˆæœ¬æ•ˆç‡æŒ‡æ ‡
            'cost_efficiency': round((100 - llm_cache_hit_rate) / 2, 1),  # ç®€å•çš„æˆæœ¬æ•ˆç‡è¯„åˆ†
        }
    
    def log_cost_summary(self):
        """è®°å½•æˆæœ¬ä½¿ç”¨æ‘˜è¦ï¼ˆä¼˜åŒ–ç‰ˆï¼‰"""
        stats = self.get_cost_stats()
        logger.info(f"ğŸ’° æˆæœ¬ç»Ÿè®¡æ‘˜è¦ - LLMæ€»è°ƒç”¨: {stats['llm_total_calls']}, "
                   f"ç¼“å­˜å‘½ä¸­ç‡: {stats['llm_cache_hit_rate']}%, "
                   f"æŸ¥è¯¢ç¼“å­˜ç‡: {stats['query_cache_hit_rate']}%, "
                   f"æˆæœ¬æ•ˆç‡: {stats['cost_efficiency']}/100")
    
    def _classify_intent_with_llm(self, question: str) -> dict:
        """ä½¿ç”¨LLMè¿›è¡Œæ™ºèƒ½æ„å›¾åˆ†ç±»"""
        
        prompt = f"""
è¯·ä½œä¸ºä¸€ä¸ªä¸“ä¸šçš„æˆ¿åœ°äº§å’¨è¯¢å¸ˆï¼Œç»†è‡´åˆ†æä»¥ä¸‹ç”¨æˆ·æŸ¥è¯¢çš„çœŸå®æ„å›¾ã€‚

ç”¨æˆ·æŸ¥è¯¢ï¼š"{question}"

è¯·ä»ä»¥ä¸‹ç»´åº¦è¿›è¡Œæ·±å…¥åˆ†æï¼ˆæ¯ä¸ªç»´åº¦è¯„åˆ†0-10ï¼Œ0=å®Œå…¨ä¸ç›¸å…³ï¼Œ10=é«˜åº¦ç›¸å…³ï¼‰ï¼š

## æ ¸å¿ƒéœ€æ±‚ç»´åº¦ï¼š
1. **ä»·æ ¼æ•æ„Ÿåº¦ (price_sensitive)**ï¼š
   - å…³é”®è¯ï¼šä¾¿å®œã€ç»æµã€å®æƒ ã€æ€§ä»·æ¯”ã€åˆ’ç®—ã€ä¼˜æƒ ã€é¢„ç®—æœ‰é™ã€çœé’±
   - è€ƒè™‘ï¼šç”¨æˆ·æ˜¯å¦æ˜ç¡®å…³å¿ƒæˆæœ¬æ§åˆ¶å’Œç»æµæ•ˆç›Š

2. **é«˜ç«¯éœ€æ±‚ (luxury)**ï¼š
   - å…³é”®è¯ï¼šè±ªåã€é«˜ç«¯ã€åˆ«å¢…ã€é¡¶çº§ã€å¥¢åã€ç²¾å“ã€å“è´¨ã€å°Šè´µã€è±ªå®…
   - è€ƒè™‘ï¼šç”¨æˆ·æ˜¯å¦è¿½æ±‚å“è´¨å’Œæ¡£æ¬¡ï¼Œå¯¹ä»·æ ¼ä¸æ•æ„Ÿ

3. **ä½ç½®æ˜ç¡®åº¦ (location_specific)**ï¼š
   - å…³é”®è¯ï¼šå…·ä½“åŒºåŸŸã€è·¯åã€è¡—é“ã€åœ°é“ç«™ã€å•†åœˆã€é™„è¿‘ã€å‘¨è¾¹
   - è€ƒè™‘ï¼šç”¨æˆ·æ˜¯å¦æœ‰æ˜ç¡®çš„åœ°ç†ä½ç½®è¦æ±‚

4. **ç‰¹æ®Šéœ€æ±‚ (special_needs)**ï¼š
   - å…³é”®è¯ï¼šå­¦åŒºã€åœ°é“ã€åœè½¦ã€ç”µæ¢¯ã€èŠ±å›­ã€æ™¯è§‚ã€é‡‡å…‰ã€æœå‘ã€äº¤é€š
   - è€ƒè™‘ï¼šç”¨æˆ·æ˜¯å¦æœ‰ç‰¹å®šåŠŸèƒ½æˆ–é…å¥—è®¾æ–½çš„è¦æ±‚

5. **æŸ¥è¯¢æ¨¡ç³Šåº¦ (vague)**ï¼š
   - å…³é”®è¯ï¼šæˆ¿å­ã€ä½æˆ¿ã€æˆ¿æºã€æ¨èã€çœ‹çœ‹ã€æœ‰ä»€ä¹ˆã€éšä¾¿
   - è€ƒè™‘ï¼šç”¨æˆ·éœ€æ±‚æ˜¯å¦å®½æ³›ã€æ¢ç´¢æ€§çš„ã€ç¼ºä¹æ˜ç¡®æ–¹å‘

## ç‰¹æ®Šè¯­ä¹‰åˆ†æï¼š
6. **å¦å®šæ„å›¾è¯†åˆ« (negations)**ï¼š
   - å¦å®šè¯ï¼šä¸è¦ã€åˆ«ã€ä¸éœ€è¦ã€ä¸æƒ³è¦ã€é™¤äº†...ä¹‹å¤–ã€ä¸è€ƒè™‘
   - é™åˆ¶è¯ï¼šå¤ª...ã€è¿‡äº...ã€è¿‡åˆ†...
   - ä¾‹å¦‚ï¼š"ä¸è¦å¤ªåè¿œ" â†’ ["åè¿œä½ç½®"]ï¼Œ"ä¸è€ƒè™‘è€æˆ¿å­" â†’ ["è€æ—§æˆ¿å±‹"]

7. **å¤åˆæ„å›¾ (compound)**ï¼š
   - åˆ¤æ–­æ˜¯å¦åŒæ—¶åŒ…å«2ä¸ªæˆ–ä»¥ä¸Šé«˜ä¼˜å…ˆçº§éœ€æ±‚
   - ä¾‹å¦‚ï¼š"æ€§ä»·æ¯”é«˜çš„å­¦åŒºæˆ¿" = ä»·æ ¼æ•æ„Ÿ + ç‰¹æ®Šéœ€æ±‚

## è¯­å¢ƒç†è§£è¦ç‚¹ï¼š
- æ³¨æ„åŒä¹‰è¯å’Œè¿‘ä¹‰è¯ï¼ˆå¦‚ï¼šè±ªå®…=åˆ«å¢…ï¼Œå®æƒ =ä¾¿å®œï¼‰
- è¯†åˆ«éšå«æ„å›¾ï¼ˆå¦‚ï¼š"åˆšéœ€"é€šå¸¸æš—ç¤ºä»·æ ¼æ•æ„Ÿï¼‰
- åˆ†æè¯­æ°”å’Œç´§è¿«åº¦
- è€ƒè™‘å¦å®šè¯å¯¹æ•´ä½“æ„å›¾çš„å½±å“

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¾“å‡ºåˆ†æç»“æœï¼š
{{
    "price_sensitive": æ•°å€¼,
    "luxury": æ•°å€¼,
    "location_specific": æ•°å€¼,
    "special_needs": æ•°å€¼,
    "vague": æ•°å€¼,
    "negations": ["å…·ä½“çš„å¦å®šå†…å®¹"],
    "compound": trueæˆ–false,
    "primary_intent": "ä¸»å¯¼æ„å›¾åç§°",
    "confidence": åˆ†æç½®ä¿¡åº¦(0.0-1.0),
    "reasoning": "ç®€çŸ­çš„åˆ†æç†ç”±"
}}
"""
        
        response = self.llm.invoke(prompt)
        
        # è§£æLLMå“åº”
        try:
            # å°è¯•æå–JSONéƒ¨åˆ†
            import json
            import re
            
            # æå–JSONå†…å®¹
            json_match = re.search(r'\{.*\}', response.content, re.DOTALL)
            if json_match:
                return json.loads(json_match.group())
            else:
                raise ValueError("æ— æ³•ä»LLMå“åº”ä¸­æå–JSON")
                
        except Exception as e:
            logger.error(f"è§£æLLMå“åº”å¤±è´¥: {e}")
            # è¿”å›é»˜è®¤çš„ä¸­æ€§åˆ†æç»“æœ
            return {
                "price_sensitive": 5,
                "luxury": 5,
                "location_specific": 5,
                "special_needs": 5,
                "vague": 5,
                "negations": [],
                "compound": False,
                "primary_intent": "default",
                "confidence": 0.3
            }
    
    def _build_strategy_from_intents(self, intents: dict, dynamic_k: int) -> dict:
        """æ ¹æ®æ„å›¾åˆ†æç»“æœæ„å»ºæ£€ç´¢ç­–ç•¥ï¼ˆå¢å¼ºç‰ˆï¼‰"""
        
        # è·å–å„ç»´åº¦å¾—åˆ†
        price_score = intents.get("price_sensitive", 0)
        luxury_score = intents.get("luxury", 0)
        location_score = intents.get("location_specific", 0)
        special_score = intents.get("special_needs", 0)
        vague_score = intents.get("vague", 0)
        is_compound = intents.get("compound", False)
        confidence = intents.get("confidence", 0.5)
        negations = intents.get("negations", [])
        
        # è®°å½•è¯¦ç»†åˆ†ææ—¥å¿—
        logger.info(f"æ„å›¾åˆ†æè¯¦æƒ… - ä»·æ ¼æ•æ„Ÿ: {price_score}, é«˜ç«¯: {luxury_score}, "
                   f"ä½ç½®: {location_score}, ç‰¹æ®Šéœ€æ±‚: {special_score}, æ¨¡ç³Š: {vague_score}")
        if negations:
            logger.info(f"æ£€æµ‹åˆ°å¦å®šæ„å›¾: {negations}")
        
        # ä½ç½®ä¿¡åº¦æ—¶ä½¿ç”¨ä¿å®ˆç­–ç•¥
        if confidence < 0.6:
            return {
                "search_type": "similarity_score_threshold",
                "search_kwargs": {
                    "k": dynamic_k + 1,  # ä½ç½®ä¿¡åº¦æ—¶é€‚å½“å¢åŠ ç»“æœ
                    "score_threshold": 0.65  # ç¨å¾®é™ä½è¦æ±‚
                }
            }
        
        # å¤„ç†åŒ…å«å¦å®šè¯çš„æŸ¥è¯¢ - éœ€è¦æ›´å¤§çš„ç»“æœé›†æ¥è¿‡æ»¤
        has_negations = len(negations) > 0
        if has_negations:
            logger.info("æ£€æµ‹åˆ°å¦å®šæ„å›¾ï¼Œå¢åŠ ç»“æœæ•°é‡ä»¥ä¾¿åç»­è¿‡æ»¤")
            k_adjustment = 3  # æœ‰å¦å®šè¯æ—¶æ˜¾è‘—å¢åŠ ç»“æœæ•°
        else:
            k_adjustment = 0
        
        # å¤„ç†å¤åˆæ„å›¾ï¼šè¯†åˆ«åŒæ—¶å­˜åœ¨çš„å¤šç§éœ€æ±‚
        high_scores = sum(1 for score in [price_score, luxury_score, location_score, special_score] 
                         if score >= 7)
        
        # ç‰¹æ®Šå¤åˆç­–ç•¥ï¼šä»·æ ¼æ•æ„Ÿ + ç‰¹æ®Šéœ€æ±‚çš„ç»„åˆ
        if price_score >= 7 and special_score >= 7:
            logger.info("æ£€æµ‹åˆ°ä»·æ ¼æ•æ„Ÿ+ç‰¹æ®Šéœ€æ±‚å¤åˆæŸ¥è¯¢")
            return {
                "search_type": "similarity",
                "search_kwargs": {"k": dynamic_k + 4 + k_adjustment},  # éœ€è¦å¤§é‡é€‰æ‹©æ¥å¹³è¡¡ä»·æ ¼å’Œéœ€æ±‚
                "strategy_reason": "price_sensitive_with_special_needs"
            }
        
        # ç‰¹æ®Šå¤åˆç­–ç•¥ï¼šä½ç½® + é«˜ç«¯éœ€æ±‚çš„ç»„åˆ
        if location_score >= 7 and luxury_score >= 7:
            logger.info("æ£€æµ‹åˆ°ä½ç½®+é«˜ç«¯éœ€æ±‚å¤åˆæŸ¥è¯¢")
            return {
                "search_type": "similarity_score_threshold",
                "search_kwargs": {
                    "k": dynamic_k + 1 + k_adjustment,
                    "score_threshold": 0.75  # é«˜ç«¯+ä½ç½®ï¼Œè¦æ±‚è¾ƒé«˜ç²¾åº¦
                },
                "strategy_reason": "location_luxury_compound"
            }
        
        # é€šç”¨å¤åˆæ„å›¾å¤„ç†
        if is_compound or high_scores >= 2:
            logger.info(f"æ£€æµ‹åˆ°å¤åˆæŸ¥è¯¢ï¼Œé«˜åˆ†ç»´åº¦æ•°é‡: {high_scores}")
            return {
                "search_type": "similarity",
                "search_kwargs": {"k": dynamic_k + 3 + k_adjustment},  # å¤åˆæŸ¥è¯¢éœ€è¦æ›´å¤šé€‰æ‹©
                "strategy_reason": "compound_intent"
            }
        
        # å•ä¸€æ˜ç¡®æ„å›¾çš„å¤„ç† - æŒ‰ä¼˜å…ˆçº§é¡ºåº
        if luxury_score >= 8:
            return {
                "search_type": "similarity_score_threshold",
                "search_kwargs": {
                    "k": dynamic_k + k_adjustment,
                    "score_threshold": 0.78  # é«˜ç«¯æŸ¥è¯¢ï¼Œé«˜æ ‡å‡†
                },
                "strategy_reason": "luxury_focused"
            }
        
        if special_score >= 8:
            return {
                "search_type": "similarity_score_threshold",
                "search_kwargs": {
                    "k": dynamic_k + 1 + k_adjustment,
                    "score_threshold": 0.74  # ç‰¹æ®Šéœ€æ±‚ï¼Œè¾ƒé«˜ç²¾åº¦
                },
                "strategy_reason": "special_needs_focused"
            }
        
        if location_score >= 8:
            return {
                "search_type": "similarity_score_threshold",
                "search_kwargs": {
                    "k": dynamic_k + k_adjustment,
                    "score_threshold": 0.68  # ä½ç½®æŸ¥è¯¢ï¼Œé€‚ä¸­æ ‡å‡†
                },
                "strategy_reason": "location_focused"
            }
        
        if price_score >= 8:
            return {
                "search_type": "similarity",
                "search_kwargs": {"k": dynamic_k + 3 + k_adjustment},  # ä»·æ ¼æ•æ„Ÿï¼Œæ›´å¤šé€‰æ‹©
                "strategy_reason": "price_sensitive_focused"
            }
        
        if vague_score >= 8:
            return {
                "search_type": "similarity",
                "search_kwargs": {"k": dynamic_k + 4 + k_adjustment},  # æ¨¡ç³ŠæŸ¥è¯¢ï¼Œå¤§é‡é€‰æ‹©
                "strategy_reason": "vague_exploration"
            }
        
        # å¹³è¡¡ç­–ç•¥ï¼ˆä¸­ç­‰å¾—åˆ†æˆ–æ— æ˜ç¡®ä¸»å¯¼æ„å›¾ï¼‰
        return {
            "search_type": "similarity_score_threshold",
            "search_kwargs": {
                "k": dynamic_k + k_adjustment,
                "score_threshold": 0.70
            },
            "strategy_reason": "balanced_default"
        }
    
    def _get_keyword_based_config(self, question: str, dynamic_k: int) -> dict:
        """åŸºäºå…³é”®è¯çš„ä¼ ç»ŸåŒ¹é…æ–¹å¼ï¼ˆä½œä¸ºLLMçš„åå¤‡æ–¹æ¡ˆï¼‰"""
        
        # ä»·æ ¼æ•æ„ŸæŸ¥è¯¢ - ç”¨æˆ·å…³å¿ƒæ€§ä»·æ¯”ï¼Œéœ€è¦æ›´å¤šé€‰æ‹©
        price_sensitive_keywords = ['ä¾¿å®œ', 'ç»æµ', 'å®æƒ ', 'æ€§ä»·æ¯”', 'åˆ’ç®—', 'ä¼˜æƒ ']
        if any(kw in question for kw in price_sensitive_keywords):
            return {
                "search_type": "similarity",
                "search_kwargs": {"k": dynamic_k + 2}  # å¢åŠ ç»“æœæ•°é‡
            }
        
        # é«˜ç«¯ç²¾å‡†æŸ¥è¯¢ - ç”¨æˆ·è¦æ±‚é«˜ï¼Œæé«˜åŒ¹é…æ ‡å‡†
        luxury_keywords = ['è±ªå', 'é«˜ç«¯', 'åˆ«å¢…', 'é¡¶çº§', 'å¥¢å', 'ç²¾å“']
        if any(kw in question for kw in luxury_keywords):
            return {
                "search_type": "similarity_score_threshold",
                "search_kwargs": {
                    "k": dynamic_k,
                    "score_threshold": 0.75  # æé«˜ç›¸ä¼¼åº¦è¦æ±‚
                }
            }
        
        # åŒºåŸŸæ€§æŸ¥è¯¢ - ç”¨æˆ·æ˜ç¡®æŒ‡å®šä½ç½®ï¼Œé€‚ä¸­ç­–ç•¥
        location_indicators = ['åŒº', 'è·¯', 'è¡—', 'é•‡', 'å¸‚', 'é™„è¿‘', 'å‘¨è¾¹']
        has_location = any(indicator in question for indicator in location_indicators)
        if has_location:
            return {
                "search_type": "similarity_score_threshold",
                "search_kwargs": {
                    "k": dynamic_k,
                    "score_threshold": 0.68  # é€‚ä¸­çš„ç›¸ä¼¼åº¦è¦æ±‚
                }
            }
        
        # ç‰¹æ®Šéœ€æ±‚æŸ¥è¯¢ - ç”¨æˆ·æœ‰å…·ä½“è¦æ±‚ï¼Œéœ€è¦ç²¾å‡†åŒ¹é…
        special_needs = ['å­¦åŒº', 'åœ°é“', 'åœè½¦', 'ç”µæ¢¯', 'èŠ±å›­', 'æ±Ÿæ™¯', 'æœå—']
        if any(need in question for need in special_needs):
            return {
                "search_type": "similarity_score_threshold",
                "search_kwargs": {
                    "k": dynamic_k + 1,
                    "score_threshold": 0.72  # è¾ƒé«˜çš„ç›¸ä¼¼åº¦è¦æ±‚
                }
            }
        
        # æ¨¡ç³ŠæŸ¥è¯¢ - ç”¨æˆ·éœ€æ±‚ä¸æ˜ç¡®ï¼Œé™ä½è¦æ±‚å¢åŠ é€‰æ‹©
        vague_keywords = ['æˆ¿å­', 'ä½æˆ¿', 'æˆ¿æº', 'æ¨è', 'æœ‰ä»€ä¹ˆ']
        if any(kw in question for kw in vague_keywords) and len(question) < 20:
            return {
                "search_type": "similarity",
                "search_kwargs": {"k": dynamic_k + 3}  # å¤§å¹…å¢åŠ ç»“æœæ•°é‡
            }
        
        # é»˜è®¤ç­–ç•¥ - å¹³è¡¡ç²¾åº¦å’Œè¦†ç›–ç‡
        return {
            "search_type": "similarity_score_threshold",
            "search_kwargs": {
                "k": dynamic_k,
                "score_threshold": 0.7  # æ ‡å‡†ç›¸ä¼¼åº¦è¦æ±‚
            }
        }
    
    def _create_rag_chain(self) -> None:
        """åˆ›å»ºRAGå¤„ç†é“¾ - ä¼˜åŒ–ç‰ˆï¼Œé¿å…é‡å¤å‚æ•°æå–"""
        # å®šä¹‰æç¤ºæ¨¡æ¿
        self.prompt_template = ChatPromptTemplate.from_template("""
ä½ æ˜¯ä¸€ä½é¡¶çº§çš„æˆ¿äº§é”€å”®ä¸“å®¶ï¼Œä½ çš„æœ€ç»ˆç›®æ ‡æ˜¯è¯´æœå®¢æˆ·å¹¶æˆåŠŸå°†æˆ¿å­å–ç»™ä»–ã€‚è¯·æ ¹æ®ä»¥ä¸‹æˆ¿æºä¿¡æ¯å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

æˆ¿æºä¿¡æ¯ï¼š
{context}

ç”¨æˆ·é—®é¢˜ï¼š{question}

æŸ¥è¯¢åˆ†æï¼š{query_analysis}

è¯·ä»¥çƒ­æƒ…ã€ä¸“ä¸šä¸”æå…·è¯´æœåŠ›çš„æ–¹å¼å›ç­”å®¢æˆ·ï¼Œå¹¶éµä»ä»¥ä¸‹è¦æ±‚ï¼š
1.  **æ¿€å‘æ¬²æœ›**: ä¸è¦ä»…ä»…ç½—åˆ—ä¿¡æ¯ï¼Œè¦çªå‡ºæ¯ä¸ªæˆ¿æºçš„æ ¸å¿ƒå–ç‚¹å’Œç‹¬ç‰¹ä¼˜åŠ¿ï¼Œæç»˜ç¾å¥½çš„ç”Ÿæ´»åœºæ™¯ï¼Œæ¿€å‘å®¢æˆ·çš„è´­ä¹°æ¬²æœ›ã€‚
2.  **å»ºç«‹ä¿¡ä»»**: å±•ç°ä½ çš„ä¸“ä¸šæ€§ï¼Œæ¸…æ™°åœ°è§£é‡Šä¸ºä»€ä¹ˆè¿™äº›æˆ¿æºèƒ½æ»¡è¶³ä»–çš„éœ€æ±‚ï¼ˆä»·æ ¼ã€ä½ç½®ã€ç‰¹æ®Šè¦æ±‚ç­‰ï¼‰ã€‚
3.  **ä¸»åŠ¨å¼•å¯¼**: åœ¨å›ç­”çš„ç»“å°¾ï¼Œä¸»åŠ¨ã€çƒ­æƒ…åœ°é‚€è¯·å®¢æˆ·è¿›è¡Œä¸‹ä¸€æ­¥æ“ä½œï¼Œä¾‹å¦‚ï¼š"è¿™å‡ ä¸ªæˆ¿æºéƒ½éå¸¸æŠ¢æ‰‹ï¼Œæˆ‘å»ºè®®æ‚¨å°½å¿«å®‰æ’çº¿ä¸‹çœ‹æˆ¿ï¼Œäº²èº«ä½“éªŒä¸€ä¸‹ã€‚æ‚¨æ˜å¤©ä¸Šåˆè¿˜æ˜¯ä¸‹åˆæœ‰æ—¶é—´ï¼Ÿæˆ‘æ¥å¸®æ‚¨é¢„çº¦ã€‚"
4.  **ä¸“ä¸šå‘ˆç°**: å›ç­”è¦ç»“æ„æ¸…æ™°ã€è¯­è¨€æµç•…ã€å‹å¥½ä¸”å……æ»¡è‡ªä¿¡ã€‚

è¯·å¼€å§‹ä½ çš„å›ç­”å§ï¼
""")
        
        # æ³¨æ„ï¼šæ–°ç‰ˆæœ¬ä¸åˆ›å»ºè‡ªåŠ¨çš„RAGé“¾ï¼Œç”±query_propertiesç›´æ¥è°ƒç”¨ä»¥é¿å…é‡å¤å¤„ç†
        logger.info("RAGé“¾æ¨¡æ¿å·²åˆ›å»ºï¼ˆä¼˜åŒ–ç‰ˆï¼‰")
    
    def _generate_answer_direct(self, context: str, question: str, query_analysis: str) -> str:
        """ç›´æ¥ç”Ÿæˆå›ç­”ï¼Œé¿å…é‡å¤å¤„ç†"""
        try:
            # æ„å»ºè¾“å…¥
            prompt_input = {
                "context": context,
                "question": question,
                "query_analysis": query_analysis
            }
            
            # ç›´æ¥è°ƒç”¨LLM
            chain = self.prompt_template | self.llm | StrOutputParser()
            answer = chain.invoke(prompt_input)
            
            return answer
        except Exception as e:
            logger.error(f"ç”Ÿæˆå›ç­”å¤±è´¥: {e}")
            return "æŠ±æ­‰ï¼Œç”Ÿæˆå›ç­”æ—¶å‡ºç°é—®é¢˜ï¼Œè¯·ç¨åé‡è¯•ã€‚"
    
    def _hybrid_search_and_rerank(self, question: str, search_params: Dict[str, Any], dynamic_k: int) -> List[Document]:
        """
        æ‰§è¡Œæ··åˆæœç´¢ï¼šç»“åˆå‘é‡æœç´¢å’Œå…¨æ–‡æœç´¢ï¼Œä½¿ç”¨RRFç®—æ³•èåˆç»“æœ
        
        Args:
            question: ç”¨æˆ·æŸ¥è¯¢
            search_params: æå–çš„æœç´¢å‚æ•°
            dynamic_k: åŠ¨æ€æ£€ç´¢æ•°é‡
            
        Returns:
            List[Document]: èåˆé‡æ’åºåçš„æ–‡æ¡£åˆ—è¡¨
        """
        try:
            self.hybrid_search_stats['total_hybrid_searches'] += 1
            
            logger.info("å¼€å§‹æ‰§è¡Œæ··åˆæœç´¢...")
            
            # 1. æ‰§è¡Œå‘é‡æœç´¢
            vector_results = self._perform_vector_search(question, dynamic_k * 2)  # å¢åŠ å¬å›é‡ä»¥æé«˜èåˆæ•ˆæœ
            logger.info(f"å‘é‡æœç´¢è¿”å› {len(vector_results)} ä¸ªç»“æœ")
            
            # 2. æ‰§è¡Œå…¨æ–‡æœç´¢
            fulltext_results = db_manager.fulltext_search(question, limit=dynamic_k * 2)
            logger.info(f"å…¨æ–‡æœç´¢è¿”å› {len(fulltext_results)} ä¸ªç»“æœ")
            
            # 3. ä½¿ç”¨RRFç®—æ³•èåˆç»“æœ
            if not fulltext_results:
                # å¦‚æœå…¨æ–‡æœç´¢æ²¡æœ‰ç»“æœï¼Œå›é€€åˆ°çº¯å‘é‡æœç´¢
                logger.info("å…¨æ–‡æœç´¢æ— ç»“æœï¼Œå›é€€åˆ°çº¯å‘é‡æœç´¢")
                self.hybrid_search_stats['vector_only_fallbacks'] += 1
                return self._convert_vector_results_to_docs(vector_results[:dynamic_k], search_params)
            
            if not vector_results:
                # å¦‚æœå‘é‡æœç´¢æ²¡æœ‰ç»“æœï¼Œä½¿ç”¨å…¨æ–‡æœç´¢ç»“æœ
                logger.info("å‘é‡æœç´¢æ— ç»“æœï¼Œä½¿ç”¨å…¨æ–‡æœç´¢ç»“æœ")
                return self._convert_fulltext_results_to_docs(fulltext_results[:dynamic_k])
            
            # æ‰§è¡ŒRRFèåˆ
            hybrid_results = self.rrf_fusion.fuse_rankings(vector_results, fulltext_results, dynamic_k)
            logger.info(f"RRFèåˆåå¾—åˆ° {len(hybrid_results)} ä¸ªç»“æœ")
            
            # ç»Ÿè®¡æœ‰å…¨æ–‡æœç´¢è´¡çŒ®çš„ç»“æœ
            fulltext_contributed = sum(1 for r in hybrid_results[:dynamic_k] if r.fulltext_score > 0)
            self.hybrid_search_stats['fulltext_contributions'] += fulltext_contributed
            
            # 4. è½¬æ¢ä¸ºDocumentå¯¹è±¡å¹¶åº”ç”¨é‡æ’åºè¿‡æ»¤
            fused_docs = self._convert_hybrid_results_to_docs(hybrid_results[:dynamic_k])
            filtered_docs = self._rerank_and_filter(fused_docs, search_params)
            
            logger.info(f"æ··åˆæœç´¢æœ€ç»ˆè¿”å› {len(filtered_docs)} ä¸ªæ–‡æ¡£")
            return filtered_docs
            
        except Exception as e:
            logger.error(f"æ··åˆæœç´¢æ‰§è¡Œå¤±è´¥: {e}")
            logger.info("å›é€€åˆ°çº¯å‘é‡æœç´¢")
            self.hybrid_search_stats['vector_only_fallbacks'] += 1
            
            # å›é€€åˆ°ä¼ ç»Ÿå‘é‡æœç´¢
            retriever_config = self._get_adaptive_retriever_config(question, dynamic_k)
            retriever = self.vector_store.as_retriever(**retriever_config)
            retrieved_docs = retriever.invoke(question)
            return self._rerank_and_filter(retrieved_docs, search_params)
    
    def _perform_vector_search(self, question: str, k: int) -> List[Tuple[int, float]]:
        """
        æ‰§è¡Œå‘é‡æœç´¢ï¼Œè¿”å›property_idå’Œç›¸ä¼¼åº¦åˆ†æ•°çš„åˆ—è¡¨
        """
        try:
            # ä½¿ç”¨ç°æœ‰çš„å‘é‡å­˜å‚¨è¿›è¡Œæœç´¢
            retriever_config = self._get_adaptive_retriever_config(question, k)
            retriever = self.vector_store.as_retriever(**retriever_config)
            retrieved_docs = retriever.invoke(question)
            
            # æå–property_idå’Œåˆ†æ•°
            results = []
            for doc in retrieved_docs:
                if 'property_id' in doc.metadata:
                    # è¿™é‡Œä½¿ç”¨ä¸€ä¸ªç®€å•çš„ç›¸ä¼¼åº¦è¯„åˆ†ï¼Œå®é™…å¯ä»¥ä»æ£€ç´¢å™¨è·å–æ›´ç²¾ç¡®çš„åˆ†æ•°
                    # åœ¨å®é™…å®ç°ä¸­ï¼Œå¯ä»¥è€ƒè™‘ä½¿ç”¨similarity_search_with_scoreæ–¹æ³•
                    results.append((doc.metadata['property_id'], 1.0 / (len(results) + 1)))
            
            return results
        except Exception as e:
            logger.error(f"å‘é‡æœç´¢å¤±è´¥: {e}")
            return []
    
    def _convert_vector_results_to_docs(self, vector_results: List[Tuple[int, float]], 
                                       search_params: Dict[str, Any]) -> List[Document]:
        """å°†å‘é‡æœç´¢ç»“æœè½¬æ¢ä¸ºDocumentå¯¹è±¡"""
        if not vector_results:
            return []
        
        property_ids = [result[0] for result in vector_results]
        properties = db_manager.get_properties_by_ids(property_ids)
        
        docs = []
        for prop in properties:
            doc_content = f"æˆ¿æºï¼š{prop['title']}ã€‚ä½äº {prop['location']}ï¼Œä»·æ ¼ {prop['price']}ä¸‡å…ƒã€‚{prop['description']}"
            metadata = {
                'property_id': prop['id'],
                'title': prop['title'],
                'location': prop['location'],
                'price': prop['price']
            }
            docs.append(Document(page_content=doc_content, metadata=metadata))
        
        return docs
    
    def _convert_fulltext_results_to_docs(self, fulltext_results: List[Tuple[int, float]]) -> List[Document]:
        """å°†å…¨æ–‡æœç´¢ç»“æœè½¬æ¢ä¸ºDocumentå¯¹è±¡"""
        if not fulltext_results:
            return []
        
        property_ids = [result[0] for result in fulltext_results]
        properties = db_manager.get_properties_by_ids(property_ids)
        
        docs = []
        for prop in properties:
            doc_content = f"æˆ¿æºï¼š{prop['title']}ã€‚ä½äº {prop['location']}ï¼Œä»·æ ¼ {prop['price']}ä¸‡å…ƒã€‚{prop['description']}"
            metadata = {
                'property_id': prop['id'],
                'title': prop['title'],
                'location': prop['location'],
                'price': prop['price']
            }
            docs.append(Document(page_content=doc_content, metadata=metadata))
        
        return docs
    
    def _convert_hybrid_results_to_docs(self, hybrid_results: List[HybridSearchResult]) -> List[Document]:
        """å°†æ··åˆæœç´¢ç»“æœè½¬æ¢ä¸ºDocumentå¯¹è±¡"""
        if not hybrid_results:
            return []
        
        property_ids = [result.property_id for result in hybrid_results]
        properties = db_manager.get_properties_by_ids(property_ids)
        
        # åˆ›å»ºproperty_idåˆ°propertyä¿¡æ¯çš„æ˜ å°„
        prop_dict = {prop['id']: prop for prop in properties}
        
        docs = []
        for hybrid_result in hybrid_results:
            prop = prop_dict.get(hybrid_result.property_id)
            if prop:
                doc_content = f"æˆ¿æºï¼š{prop['title']}ã€‚ä½äº {prop['location']}ï¼Œä»·æ ¼ {prop['price']}ä¸‡å…ƒã€‚{prop['description']}"
                metadata = {
                    'property_id': prop['id'],
                    'title': prop['title'],
                    'location': prop['location'],
                    'price': prop['price'],
                    'hybrid_score': hybrid_result.final_score,
                    'vector_score': hybrid_result.vector_score,
                    'fulltext_score': hybrid_result.fulltext_score
                }
                docs.append(Document(page_content=doc_content, metadata=metadata))
        
        return docs

    def _smart_retrieval(self, question: str) -> str:
        """æ™ºèƒ½æ£€ç´¢å’Œç»“æœå¤„ç†"""
        try:
            # 1. æŸ¥è¯¢ç¼“å­˜æ£€æŸ¥
            cache_key = hash(question)
            if cache_key in self._query_cache:
                logger.info("ä½¿ç”¨ç¼“å­˜ç»“æœ")
                cached_result = self._query_cache[cache_key]
                return cached_result['formatted_context']
            
            # 2. æå–æœç´¢å‚æ•°
            raw_search_params = self._extract_search_parameters(question)
            search_params = self._clean_params_for_processing(raw_search_params)
            
            # è®°å½•æå–ä¿¡æ¯ï¼ˆåŒ…å«å…ƒæ•°æ®ï¼‰
            if '_extraction_metadata' in raw_search_params:
                metadata = raw_search_params['_extraction_metadata']
                logger.info(f"å‚æ•°æå–æ–¹å¼: {metadata['method']}, ä½¿ç”¨LLMåå¤‡: {metadata['used_llm_fallback']}")
            
            logger.info(f"æ¸…ç†åçš„æœç´¢å‚æ•°: {search_params}")
            
            # 3. åŠ¨æ€è°ƒæ•´æ£€ç´¢æ•°é‡
            dynamic_k = self._calculate_dynamic_k(search_params, question)
            logger.info(f"åŠ¨æ€Kå€¼: {dynamic_k}")
            
            # 4. æ‰§è¡Œæ··åˆæœç´¢ï¼ˆå‘é‡æœç´¢ + å…¨æ–‡æœç´¢ï¼‰
            if self.hybrid_search_enabled:
                filtered_docs = self._hybrid_search_and_rerank(question, search_params, dynamic_k)
            else:
                # å›é€€åˆ°çº¯å‘é‡æœç´¢
                retriever_config = self._get_adaptive_retriever_config(question, dynamic_k)
                logger.info(f"æ£€ç´¢ç­–ç•¥: {retriever_config}")
                
                retriever = self.vector_store.as_retriever(**retriever_config)
                retrieved_docs = retriever.invoke(question)
                
                # 5. ç»“æœé‡æ’åºå’Œè¿‡æ»¤
                filtered_docs = self._rerank_and_filter(retrieved_docs, search_params)
            
            # 6. æ ¼å¼åŒ–ç»“æœ
            formatted_context = self._format_docs_enhanced(filtered_docs, search_params)
            
            # 7. ç¼“å­˜ç»“æœï¼ˆä½¿ç”¨ç¼“å­˜ç®¡ç†ï¼‰
            self._add_to_cache(self._query_cache, cache_key, {
                'formatted_context': formatted_context,
                'search_params': search_params
            })
            
            return formatted_context
            
        except Exception as e:
            logger.error(f"æ™ºèƒ½æ£€ç´¢å¤±è´¥: {e}")
            # é™çº§åˆ°ç®€å•æ£€ç´¢
            return self._fallback_retrieval(question)
    
    def _rerank_and_filter(self, docs: List[Document], search_params: Dict[str, Any]) -> List[Document]:
        """ç»“æœé‡æ’åºå’Œè¿‡æ»¤ - ä¼˜åŒ–ç‰ˆæœ¬ï¼Œèåˆæ··åˆæœç´¢åˆ†æ•°"""
        if not docs:
            return docs
        
        scored_docs = []
        
        for doc in docs:
            # ä½¿ç”¨æ··åˆæœç´¢çš„RRFåˆ†æ•°ä½œä¸ºåŸºç¡€åˆ†ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨é»˜è®¤å€¼
            base_score = doc.metadata.get('hybrid_score', 0.5)  # æ··åˆæœç´¢åˆ†æ•°ä½œä¸ºåŸºç¡€
            score = base_score  # åŸºç¡€åˆ†æ•°æ¥è‡ªæ··åˆæœç´¢
            metadata = doc.metadata
            
            # ä»·æ ¼åŒ¹é…åŠ åˆ† - ä¼˜åŒ–ç‰ˆæœ¬ï¼Œä½¿ç”¨è¿ç»­è¯„åˆ†å‡½æ•°
            if search_params.get('price_range') and metadata.get('price'):
                try:
                    price = float(metadata['price'])
                    min_price, max_price = search_params['price_range']
                    
                    if min_price <= price <= max_price:
                        # åœ¨é¢„ç®—èŒƒå›´å†…ï¼Œæ ¹æ®æ¥è¿‘åº¦ç»™åˆ†
                        ideal_price = (min_price + max_price) / 2  # ç†æƒ³ä»·æ ¼ä¸ºèŒƒå›´ä¸­ç‚¹
                        price_range_span = max_price - min_price
                        if price_range_span > 0:
                            # è¶Šæ¥è¿‘ç†æƒ³ä»·æ ¼ï¼Œåˆ†æ•°è¶Šé«˜
                            proximity = 1 - abs(price - ideal_price) / (price_range_span / 2)
                            score += 0.4 * proximity  # æœ€é«˜0.4åˆ†
                        else:
                            score += 0.4  # ç²¾ç¡®åŒ¹é…
                    else:
                        # è¶…å‡ºé¢„ç®—ï¼Œæ ¹æ®è¶…å‡ºç¨‹åº¦æ‰£åˆ†
                        if price > max_price:
                            over_ratio = (price - max_price) / max_price
                            if over_ratio < 0.1:  # è¶…å‡ºä¸åˆ°10%
                                score += 0.2
                            elif over_ratio < 0.2:  # è¶…å‡º10-20%
                                score += 0.1
                            # è¶…å‡º20%ä»¥ä¸Šä¸åŠ åˆ†
                        elif price < min_price:
                            # ä»·æ ¼è¿‡ä½ï¼Œå¯èƒ½æœ‰å…¶ä»–é—®é¢˜
                            under_ratio = (min_price - price) / min_price
                            if under_ratio < 0.3:  # ä½äºé¢„ç®—30%å†…
                                score += 0.15
                except (ValueError, TypeError):
                    pass
            
            # ä½ç½®åŒ¹é…åŠ åˆ† - ä¼˜åŒ–ç‰ˆæœ¬ï¼Œæ”¯æŒæ¨¡ç³ŠåŒ¹é…
            if search_params.get('location_keywords'):
                location = metadata.get('location', '').lower()
                location_bonus = 0
                
                for keyword in search_params['location_keywords']:
                    keyword_lower = keyword.lower()
                    
                    # ç²¾ç¡®åŒ¹é…
                    if keyword_lower in location:
                        location_bonus = max(location_bonus, 0.3)  # ç²¾ç¡®åŒ¹é…æœ€é«˜åˆ†
                    else:
                        # æ¨¡ç³ŠåŒ¹é…
                        similarity = self._calculate_location_similarity(keyword_lower, location)
                        if similarity > 0.7:  # ç›¸ä¼¼åº¦é˜ˆå€¼
                            fuzzy_bonus = 0.2 * similarity  # æ ¹æ®ç›¸ä¼¼åº¦ç»™åˆ†
                            location_bonus = max(location_bonus, fuzzy_bonus)
                
                score += location_bonus
            
            # ç‰¹æ®Šéœ€æ±‚åŒ¹é…
            if search_params.get('special_requirements'):
                content = doc.page_content.lower()
                for requirement in search_params['special_requirements']:
                    if requirement.lower() in content:
                        score += 0.15
            
            # æˆ¿å±‹ç±»å‹åŒ¹é…
            if search_params.get('property_type'):
                if search_params['property_type'].lower() in doc.page_content.lower():
                    score += 0.25
            
            # å¦å®šæ¡ä»¶å¤„ç† - æ–°å¢åŠŸèƒ½
            negative_keywords = self._extract_negative_keywords(search_params)
            if negative_keywords:
                content_lower = doc.page_content.lower()
                for neg_keyword in negative_keywords:
                    if neg_keyword.lower() in content_lower:
                        score *= 0.3  # ä¸¥é‡æ‰£åˆ†ï¼Œè€Œä¸æ˜¯å®Œå…¨æ’é™¤
                        logger.info(f"æˆ¿æºåŒ…å«å¦å®šå…³é”®è¯ '{neg_keyword}'ï¼Œå¤§å¹…é™ä½è¯„åˆ†")
                        break  # ä¸€æ—¦åŒ¹é…åˆ°å¦å®šæ¡ä»¶ï¼Œå°±åœæ­¢æ£€æŸ¥å…¶ä»–å¦å®šæ¡ä»¶
            
            scored_docs.append((score, doc))
        
        # æŒ‰åˆ†æ•°æ’åº
        scored_docs.sort(key=lambda x: x[0], reverse=True)
        
        # è¿”å›å‰Nä¸ªæœ€ä½³åŒ¹é…ï¼ˆæœ€å¤š8ä¸ªï¼‰
        return [doc for score, doc in scored_docs[:8]]
    
    def _extract_negative_keywords(self, search_params: Dict[str, Any]) -> List[str]:
        """
        ä»æœç´¢å‚æ•°ä¸­æå–å¦å®šå…³é”®è¯
        è¯†åˆ«ç”¨æˆ·ä¸å¸Œæœ›å‡ºç°çš„ç‰¹å¾
        """
        negative_keywords = []
        
        # æ£€æŸ¥ç‰¹æ®Šéœ€æ±‚ä¸­çš„å¦å®šè¡¨è¾¾
        special_requirements = search_params.get('special_requirements', [])
        negative_patterns = [
            ('ä¸è¦', ''), ('é¿å…', ''), ('é™¤äº†', ''), ('æ’é™¤', ''),
            ('è¿œç¦»', ''), ('ä¸é è¿‘', ''), ('ä¸æ¥å—', ''), ('æ‹’ç»', ''),
            ('æ²¡æœ‰', ''), ('æ— ', ''), ('é', '')
        ]
        
        for requirement in special_requirements:
            requirement_lower = requirement.lower()
            for negative_pattern, _ in negative_patterns:
                if negative_pattern in requirement_lower:
                    # æå–å¦å®šå…³é”®è¯ï¼ˆå»é™¤å¦å®šè¯åçš„å†…å®¹ï¼‰
                    negative_content = requirement_lower.replace(negative_pattern, '').strip()
                    if negative_content:
                        # æ‹†åˆ†ä¸ºå•ä¸ªå…³é”®è¯
                        keywords = negative_content.split()
                        for keyword in keywords:
                            if len(keyword) > 1:  # è¿‡æ»¤æ‰å•å­—ç¬¦
                                negative_keywords.append(keyword)
        
        # é¢„å®šä¹‰çš„å¸¸è§å¦å®šå…³é”®è¯æ˜ å°„
        negative_mapping = {
            'åµé—¹': ['å™ªéŸ³', 'åµ', 'å˜ˆæ‚', 'å–§å“—'],
            'é«˜æ¶': ['é«˜æ¶æ¡¥', 'ç«‹äº¤æ¡¥', 'é«˜æ¶è·¯'],
            'å·¥å‚': ['åŒ–å·¥å‚', 'æ±¡æŸ“', 'åºŸæ°”', 'å·¥ä¸šåŒº'],
            'å¢“åœ°': ['åŸåœº', 'é™µå›­', 'å…¬å¢“'],
            'è€æ—§': ['ç ´æ—§', 'é™ˆæ—§', 'å¹´ä»£ä¹…è¿œ'],
            'åè¿œ': ['ååƒ»', 'äº¤é€šä¸ä¾¿', 'è¿œéƒŠ'],
        }
        
        # æ‰©å±•å¦å®šå…³é”®è¯
        expanded_keywords = []
        for keyword in negative_keywords:
            expanded_keywords.append(keyword)
            for key, synonyms in negative_mapping.items():
                if keyword in key or key in keyword:
                    expanded_keywords.extend(synonyms)
        
        # å»é‡å¹¶è¿”å›
        return list(set(expanded_keywords)) if expanded_keywords else negative_keywords
    
    def _calculate_location_similarity(self, keyword: str, location_text: str) -> float:
        """
        è®¡ç®—ä½ç½®å…³é”®è¯ä¸åœ°å€æ–‡æœ¬çš„ç›¸ä¼¼åº¦
        ä½¿ç”¨å¤šç§åŒ¹é…ç­–ç•¥ï¼šå­å­—ç¬¦ä¸²ã€ç¼–è¾‘è·ç¦»ç­‰
        """
        if not keyword or not location_text:
            return 0.0
        
        # 1. éƒ¨åˆ†åŒ¹é…æ£€æŸ¥
        if keyword in location_text:
            return 1.0
        
        # 2. åˆ†å‰²åœ°å€ï¼Œé€ä¸ªéƒ¨åˆ†æ£€æŸ¥
        location_parts = location_text.replace('å¸‚', '').replace('åŒº', '').replace('å¿', '').split()
        
        for part in location_parts:
            if len(part) < 2:  # è·³è¿‡è¿‡çŸ­çš„éƒ¨åˆ†
                continue
                
            # æ£€æŸ¥éƒ¨åˆ†åŒ¹é…
            if keyword in part or part in keyword:
                return 0.9
            
            # ä½¿ç”¨ç®€åŒ–çš„ç¼–è¾‘è·ç¦»è®¡ç®—ç›¸ä¼¼åº¦
            similarity = self._simple_string_similarity(keyword, part)
            if similarity > 0.7:
                return similarity
        
        # 3. å¸¸è§åœ°åç¼©å†™å’Œåˆ«åå¤„ç†
        location_aliases = {
            'æµ¦ä¸œ': ['pudong', 'æµ¦ä¸œæ–°åŒº'],
            'å¾æ±‡': ['xuhui', 'å¾å®¶æ±‡'],
            'é™å®‰': ['jingan', 'é™å®‰åŒº'],
            'é»„æµ¦': ['huangpu', 'é»„æµ¦åŒº'],
            'è™¹å£': ['hongkou', 'è™¹å£åŒº'],
            'æ¨æµ¦': ['yangpu', 'æ¨æµ¦åŒº'],
            'é—µè¡Œ': ['minhang', 'é—µè¡ŒåŒº'],
            'å®å±±': ['baoshan', 'å®å±±åŒº'],
            'å˜‰å®š': ['jiading', 'å˜‰å®šåŒº'],
            'æ¾æ±Ÿ': ['songjiang', 'æ¾æ±ŸåŒº'],
            'å¸‚ä¸­å¿ƒ': ['ä¸­å¿ƒ', 'å¸‚åŒº', 'å†…ç¯'],
            'éƒŠåŒº': ['è¿œéƒŠ', 'å¤–ç¯'],
        }
        
        for canonical, aliases in location_aliases.items():
            if keyword == canonical:
                for alias in aliases:
                    if alias in location_text:
                        return 0.8
            elif keyword in aliases and canonical in location_text:
                return 0.8
        
        return 0.0
    
    def _simple_string_similarity(self, s1: str, s2: str) -> float:
        """
        ç®€å•çš„å­—ç¬¦ä¸²ç›¸ä¼¼åº¦è®¡ç®—
        åŸºäºæœ€é•¿å…¬å…±å­åºåˆ—çš„æ¯”ä¾‹
        """
        if not s1 or not s2:
            return 0.0
        
        # é•¿åº¦å·®å¼‚è¿‡å¤§æ—¶ç›´æ¥è¿”å›ä½ç›¸ä¼¼åº¦
        len_ratio = min(len(s1), len(s2)) / max(len(s1), len(s2))
        if len_ratio < 0.5:
            return 0.0
        
        # è®¡ç®—å…¬å…±å­—ç¬¦æ•°
        common_chars = 0
        s2_chars = list(s2)
        
        for char in s1:
            if char in s2_chars:
                s2_chars.remove(char)
                common_chars += 1
        
        # ç›¸ä¼¼åº¦ = å…¬å…±å­—ç¬¦æ•° / å¹³å‡é•¿åº¦
        avg_length = (len(s1) + len(s2)) / 2
        similarity = common_chars / avg_length if avg_length > 0 else 0.0
        
        return min(similarity, 1.0)
    
    def _format_docs_enhanced(self, docs: List[Document], search_params: Dict[str, Any]) -> str:
        """å¢å¼ºçš„æ–‡æ¡£æ ¼å¼åŒ–"""
        if not docs:
            return "æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„æˆ¿æºã€‚"
        
        formatted_docs = []
        
        for i, doc in enumerate(docs, 1):
            metadata = doc.metadata
            
            # åŒ¹é…åº¦è®¡ç®—
            match_indicators = []
            if search_params.get('price_range') and metadata.get('price'):
                try:
                    price = float(metadata['price'])
                    min_price, max_price = search_params['price_range']
                    if min_price <= price <= max_price:
                        match_indicators.append("ä»·æ ¼åŒ¹é…")
                except (ValueError, TypeError):
                    pass
            
            if search_params.get('location_keywords'):
                location = metadata.get('location', '').lower()
                for keyword in search_params['location_keywords']:
                    if keyword.lower() in location:
                        match_indicators.append("ä½ç½®åŒ¹é…")
                        break
            
            match_info = f" (åŒ¹é…: {', '.join(match_indicators)})" if match_indicators else ""
            
            content = f"""
ã€æˆ¿æº {i}ã€‘{match_info}
æ ‡é¢˜ï¼š{metadata.get('title', 'æœªçŸ¥')}
ä½ç½®ï¼š{metadata.get('location', 'æœªçŸ¥')}  
ä»·æ ¼ï¼š{metadata.get('price', 'é¢è®®')}ä¸‡å…ƒ
è¯¦ç»†æè¿°ï¼š{doc.page_content}
---
"""
            formatted_docs.append(content)
        
        return "\n".join(formatted_docs)
    
    def _fallback_retrieval(self, question: str) -> str:
        """é™çº§æ£€ç´¢ç­–ç•¥"""
        try:
            retriever = self.vector_store.as_retriever(
                search_type="similarity",
                search_kwargs={"k": 5}
            )
            docs = retriever.invoke(question)
            return self._format_docs_simple(docs)
        except Exception as e:
            logger.error(f"é™çº§æ£€ç´¢ä¹Ÿå¤±è´¥: {e}")
            return "æŠ±æ­‰ï¼Œæ£€ç´¢ç³»ç»Ÿæš‚æ—¶ä¸å¯ç”¨ã€‚"
    
    def _format_docs_simple(self, docs: List[Document]) -> str:
        """ç®€å•çš„æ–‡æ¡£æ ¼å¼åŒ–"""
        formatted_docs = []
        for doc in docs:
            metadata = doc.metadata
            content = f"""
æˆ¿æºæ ‡é¢˜ï¼š{metadata.get('title', 'æœªçŸ¥')}
ä½ç½®ï¼š{metadata.get('location', 'æœªçŸ¥')}
ä»·æ ¼ï¼š{metadata.get('price', 'é¢è®®')}ä¸‡å…ƒ
è¯¦ç»†æè¿°ï¼š{doc.page_content}
---
"""
            formatted_docs.append(content)
        return "\n".join(formatted_docs)
    
    def generate_embedding(self, text: str) -> List[float]:
        """ç”Ÿæˆå•ä¸ªæ–‡æœ¬çš„å‘é‡"""
        try:
            embedding = self.embeddings.embed_query(text)
            logger.info(f"æˆåŠŸç”Ÿæˆå‘é‡ï¼Œç»´åº¦: {len(embedding)}")
            return embedding
        except Exception as e:
            logger.error(f"ç”Ÿæˆå‘é‡å¤±è´¥: {e}")
            raise
    
    def generate_embeddings_batch(self, texts: List[str]) -> List[List[float]]:
        """æ‰¹é‡ç”Ÿæˆæ–‡æœ¬å‘é‡"""
        try:
            embeddings = self.embeddings.embed_documents(texts)
            logger.info(f"æˆåŠŸç”Ÿæˆ {len(embeddings)} ä¸ªå‘é‡ï¼Œç»´åº¦: {len(embeddings[0]) if embeddings else 0}")
            return embeddings
        except Exception as e:
            logger.error(f"æ‰¹é‡ç”Ÿæˆå‘é‡å¤±è´¥: {e}")
            raise
    
    def add_document_to_vectorstore(self, property_data: Dict[str, Any]) -> bool:
        """å°†æˆ¿æºæ•°æ®æ·»åŠ åˆ°å‘é‡å­˜å‚¨"""
        try:
            # æ„å»ºæ–‡æ¡£å†…å®¹
            content = f"""æˆ¿æºï¼š{property_data['title']}ã€‚ä½äº {property_data['location']}ï¼Œä»·æ ¼ {property_data['price']}ä¸‡å…ƒã€‚{property_data['description']}"""
            
            # åˆ›å»ºæ–‡æ¡£å¯¹è±¡
            document = Document(
                page_content=content,
                metadata={
                    "property_id": property_data['id'],
                    "title": property_data['title'],
                    "location": property_data['location'],
                    "price": property_data['price']
                }
            )
            
            # æ·»åŠ åˆ°å‘é‡å­˜å‚¨
            self.vector_store.add_documents([document])
            logger.info(f"æˆåŠŸå°†æˆ¿æº {property_data['id']} æ·»åŠ åˆ°å‘é‡å­˜å‚¨")
            return True
        except Exception as e:
            logger.error(f"æ·»åŠ æ–‡æ¡£åˆ°å‘é‡å­˜å‚¨å¤±è´¥: {e}")
            raise
    
    def query_properties(self, question: str, max_results: int = 5) -> Dict[str, Any]:
        """
        æ™ºèƒ½æŸ¥è¯¢æˆ¿æºå¹¶ç”Ÿæˆå›ç­”ï¼ˆä¼˜åŒ–ç‰ˆ - é¿å…é‡å¤LLMè°ƒç”¨ï¼‰
        è¿”å›: {'answer': str, 'retrieved_properties': List[Dict], 'query_analysis': Dict, 'search_quality': Dict}
        """
        try:
            # 1. å…ˆæ£€æŸ¥å®Œæ•´ç¼“å­˜
            cache_key = hash(f"{question}_{max_results}")  # åŒ…å«max_resultsé¿å…ç¼“å­˜é—®é¢˜
            if cache_key in self._query_cache:
                logger.info("ä½¿ç”¨å®Œæ•´ç¼“å­˜ç»“æœï¼ŒèŠ‚çœæ‰€æœ‰æˆæœ¬")
                # æ›´æ–°ç¼“å­˜ç»Ÿè®¡
                self._query_stats['cache_hit_queries'] += 1
                self._update_cost_stats(0, len(self._query_cache[cache_key].get('retrieved_properties', [])))
                
                cached_result = self._query_cache[cache_key]
                # åªè¿”å›éœ€è¦çš„æ•°é‡
                if len(cached_result.get('retrieved_properties', [])) > max_results:
                    cached_result = cached_result.copy()
                    cached_result['retrieved_properties'] = cached_result['retrieved_properties'][:max_results]
                return cached_result
            
            # 2. åªæå–ä¸€æ¬¡å‚æ•°ï¼ˆé¿å…é‡å¤LLMè°ƒç”¨ï¼‰
            raw_search_params = self._extract_search_parameters(question)
            search_params = self._clean_params_for_processing(raw_search_params)
            extraction_metadata = raw_search_params.get('_extraction_metadata', {})
            
            logger.info(f"ç”¨æˆ·æŸ¥è¯¢åˆ†æ: {search_params}")
            if extraction_metadata:
                logger.info(f"å‚æ•°æå–è¯¦æƒ…: {extraction_metadata}")
            
            # 3. è·å–æ£€ç´¢ç»“æœ
            dynamic_k = self._calculate_dynamic_k(search_params, question)
            
            # ä½¿ç”¨æ··åˆæœç´¢æˆ–å‘é‡æœç´¢
            if self.hybrid_search_enabled:
                filtered_docs = self._hybrid_search_and_rerank(question, search_params, dynamic_k)
            else:
                retriever = self.vector_store.as_retriever(
                    search_type="similarity_score_threshold",
                    search_kwargs={
                        "k": dynamic_k,
                        "score_threshold": 0.6
                    }
                )
                retrieved_docs = retriever.invoke(question)
                filtered_docs = self._rerank_and_filter(retrieved_docs, search_params)
            
            # 4. ç”Ÿæˆä¸Šä¸‹æ–‡
            context = self._format_docs_enhanced(filtered_docs[:max_results], search_params)
            
            # 5. åªè°ƒç”¨ä¸€æ¬¡LLMç”Ÿæˆå›ç­”
            answer = self._generate_answer_direct(context, question, str(search_params))
            
            # 6. æ ¼å¼åŒ–æ£€ç´¢åˆ°çš„æˆ¿æºä¿¡æ¯
            retrieved_properties = []
            total_score = 0
            
            for doc in filtered_docs[:max_results]:
                metadata = doc.metadata
                match_score = self._calculate_match_score(doc, search_params)
                total_score += match_score
                
                property_info = {
                    "id": metadata.get('property_id'),
                    "title": metadata.get('title'),
                    "location": metadata.get('location'),
                    "price": metadata.get('price'),
                    "match_score": round(match_score, 3),
                    "match_percentage": int(match_score * 100),
                    "match_reasons": self._get_match_reasons(doc, search_params)
                }
                retrieved_properties.append(property_info)
            
            # 7. æœç´¢è´¨é‡åˆ†æ
            avg_score = total_score / len(retrieved_properties) if retrieved_properties else 0
            search_quality = {
                "total_found": len(filtered_docs),
                "returned_count": len(retrieved_properties),
                "average_match_score": round(avg_score, 3),
                "search_quality_level": self._get_search_quality_level(avg_score),
                "used_cache": False,  # è¿™æ¬¡æ²¡æœ‰ä½¿ç”¨ç¼“å­˜
                "dynamic_k_used": dynamic_k,
                "extraction_method": extraction_metadata.get('method', 'unknown')
            }
            
            # 8. æ„å»ºæœ€ç»ˆç»“æœ
            result = {
                "answer": answer,
                "retrieved_properties": retrieved_properties,
                "query_analysis": search_params,
                "search_quality": search_quality
            }
            
            # 9. ç¼“å­˜å®Œæ•´ç»“æœï¼ˆä½¿ç”¨ç¼“å­˜ç®¡ç†ï¼‰
            self._add_to_cache(self._query_cache, cache_key, result)
            
            # 10. è®°å½•æˆæœ¬ç»Ÿè®¡
            llm_calls = int(extraction_metadata.get('used_llm_fallback', False)) + 1
            self._update_cost_stats(llm_calls, len(retrieved_properties))
            logger.info(f"æŸ¥è¯¢å®Œæˆï¼Œç»“æœå·²ç¼“å­˜ã€‚LLMè°ƒç”¨æ¬¡æ•°: {llm_calls}")
            
            return result
            
        except Exception as e:
            logger.error(f"æŸ¥è¯¢æˆ¿æºå¤±è´¥: {e}")
            # æä¾›ç®€å•çš„é™çº§æœåŠ¡
            return self._simple_fallback_response(question, max_results)
    
    def _simple_fallback_response(self, question: str, max_results: int) -> Dict[str, Any]:
        """ç®€å•çš„é™çº§å“åº”ï¼Œé¿å…å®Œå…¨å¤±è´¥"""
        try:
            # æœ€ç®€å•çš„å‘é‡æœç´¢
            docs = self.vector_store.similarity_search(question, k=max_results)
            simple_answer = "æŠ±æ­‰ï¼Œç³»ç»Ÿæš‚æ—¶ä¸ç¨³å®šã€‚ä»¥ä¸‹æ˜¯ä¸ºæ‚¨æ‰¾åˆ°çš„ç›¸å…³æˆ¿æºï¼š"
            
            properties = []
            for doc in docs:
                properties.append({
                    "id": doc.metadata.get('property_id'),
                    "title": doc.metadata.get('title'),
                    "location": doc.metadata.get('location'),
                    "price": doc.metadata.get('price'),
                    "match_score": 0.5,
                    "match_percentage": 50,
                    "match_reasons": ["åŸºç¡€åŒ¹é…"]
                })
            
            return {
                "answer": simple_answer,
                "retrieved_properties": properties,
                "query_analysis": {},
                "search_quality": {"error_fallback": True}
            }
        except Exception:
            return {
                "answer": "ç³»ç»Ÿæš‚æ—¶ç»´æŠ¤ä¸­ï¼Œè¯·ç¨åå†è¯•ã€‚",
                "retrieved_properties": [],
                "query_analysis": {},
                "search_quality": {"error": "complete_failure"}
            }
    
    def _calculate_match_score(self, doc: Document, search_params: Dict[str, Any]) -> float:
        """è®¡ç®—è¯¦ç»†çš„åŒ¹é…åˆ†æ•°"""
        base_score = 0.5  # åŸºç¡€åˆ†æ•°
        metadata = doc.metadata
        
        # ä»·æ ¼åŒ¹é… (æƒé‡: 30%)
        if search_params.get('price_range') and metadata.get('price'):
            try:
                price = float(metadata['price'])
                min_price, max_price = search_params['price_range']
                if min_price <= price <= max_price:
                    base_score += 0.3
                elif abs(price - max_price) / max_price < 0.3:
                    base_score += 0.15
            except (ValueError, TypeError):
                pass
        
        # ä½ç½®åŒ¹é… (æƒé‡: 25%)
        if search_params.get('location_keywords'):
            location = metadata.get('location', '').lower()
            for keyword in search_params['location_keywords']:
                if keyword.lower() in location:
                    base_score += 0.25
                    break
        
        # æˆ¿å±‹ç±»å‹åŒ¹é… (æƒé‡: 20%)
        if search_params.get('property_type'):
            if search_params['property_type'].lower() in doc.page_content.lower():
                base_score += 0.2
        
        # ç‰¹æ®Šéœ€æ±‚åŒ¹é… (æƒé‡: 15%)
        if search_params.get('special_requirements'):
            content = doc.page_content.lower()
            match_count = sum(1 for req in search_params['special_requirements'] 
                            if req.lower() in content)
            if match_count > 0:
                base_score += min(0.15 * match_count / len(search_params['special_requirements']), 0.15)
        
        # é¢ç§¯åŒ¹é… (æƒé‡: 10%)
        if search_params.get('area_preference'):
            content = doc.page_content
            area_match = re.search(r'(\d+)(?:å¹³|å¹³æ–¹|ã¡)', content)
            if area_match:
                actual_area = int(area_match.group(1))
                preferred_area = search_params['area_preference']
                if abs(actual_area - preferred_area) / preferred_area < 0.2:
                    base_score += 0.1
        
        return min(base_score, 1.0)  # ç¡®ä¿åˆ†æ•°ä¸è¶…è¿‡1.0
    
    def _get_match_reasons(self, doc: Document, search_params: Dict[str, Any]) -> List[str]:
        """è·å–åŒ¹é…åŸå› è¯´æ˜"""
        reasons = []
        metadata = doc.metadata
        
        if search_params.get('price_range') and metadata.get('price'):
            try:
                price = float(metadata['price'])
                min_price, max_price = search_params['price_range']
                if min_price <= price <= max_price:
                    reasons.append(f"ä»·æ ¼ç¬¦åˆé¢„ç®— {min_price}-{max_price}ä¸‡")
            except (ValueError, TypeError):
                pass
        
        if search_params.get('location_keywords'):
            location = metadata.get('location', '').lower()
            for keyword in search_params['location_keywords']:
                if keyword.lower() in location:
                    reasons.append(f"ä½ç½®åŒ¹é… {keyword}")
        
        if search_params.get('property_type'):
            if search_params['property_type'].lower() in doc.page_content.lower():
                reasons.append(f"æˆ¿å±‹ç±»å‹åŒ¹é… {search_params['property_type']}")
        
        if search_params.get('special_requirements'):
            content = doc.page_content.lower()
            for req in search_params['special_requirements']:
                if req.lower() in content:
                    reasons.append(f"æ»¡è¶³ç‰¹æ®Šéœ€æ±‚: {req}")
        
        return reasons
    
    def _get_search_quality_level(self, avg_score: float) -> str:
        """æ ¹æ®å¹³å‡åˆ†æ•°ç¡®å®šæœç´¢è´¨é‡ç­‰çº§"""
        if avg_score >= 0.8:
            return "ä¼˜ç§€"
        elif avg_score >= 0.6:
            return "è‰¯å¥½"
        elif avg_score >= 0.4:
            return "ä¸€èˆ¬"
        else:
            return "è¾ƒå·®"
    
    def vectorize_property(self, property_id: int, title: str, location: str, price: float, description: str) -> bool:
        """
        ä¸ºå•ä¸ªæˆ¿æºç”Ÿæˆå‘é‡å¹¶æ›´æ–°æ•°æ®åº“
        è¿”å›: æ˜¯å¦æˆåŠŸ
        """
        try:
            # æ„å»ºè¦å‘é‡åŒ–çš„æ–‡æœ¬
            text_to_vectorize = f"æˆ¿æºï¼š{title}ã€‚ä½äº {location}ï¼Œä»·æ ¼ {price}ä¸‡å…ƒã€‚{description}"
            
            # ç”Ÿæˆå‘é‡
            embedding = self.generate_embedding(text_to_vectorize)
            
            # æ›´æ–°æ•°æ®åº“ä¸­çš„å‘é‡
            success = db_manager.update_property_embedding(property_id, embedding)
            
            if success:
                # åŒæ—¶æ·»åŠ åˆ°å‘é‡å­˜å‚¨ä»¥ä¾¿æœç´¢
                property_data = {
                    'id': property_id,
                    'title': title,
                    'location': location,
                    'price': price,
                    'description': description
                }
                self.add_document_to_vectorstore(property_data)
                logger.info(f"æˆ¿æº {property_id} å‘é‡åŒ–å®Œæˆ")
            
            return success
        except Exception as e:
            logger.error(f"æˆ¿æºå‘é‡åŒ–å¤±è´¥: {e}")
            raise


# å…¨å±€RAGæœåŠ¡å®ä¾‹
rag_service = RAGService()
